---
title: "Wrangling Data with dplyr"
description: | 
  Making data tidying and transformation fun!
output: 
  distill::distill_article:
    toc: true
---

```{r setup, include=FALSE, purl=FALSE, message=FALSE}

library(knitr)
library(here)
suppressPackageStartupMessages(library(tidyverse))

```

:::obj

**Learning objectives**
 
 - Learn how to approach tidying and transforming datasets
 - Understand how to use the pipe (`%>%`) to chain operations together
 - Learn the core **`{dplyr}`** tools including functions (`select`, `filter`, `group_by`, `summarize`)

:::

## Data wrangling with **{dplyr}**

Data wrangling is the part of any data analysis that will take the most time. While it may not necessarily be fun, it is foundational to all the work that follows. Often, it also takes significantly longer than actually performing the data analysis or creating a data visualization, so do not panic if, in the future, you find yourself spending a lot of time on this phase. Once you learn some core approaches and tools, you can deal with nearly any dataset you may face!

(ref:AHtidyplot) *Illustration by @allison_horst, from Hadley Wickham's talk "The Joy of Functional Programming (for Data Science)"*


```{r tidywrangle, eval=TRUE, echo=FALSE, out.width='100%', fig.cap='(ref:AHtidyplot)'}

# these are all from Allison Horst: https://github.com/allisonhorst/stats-illustrations
# CITE AS: "Artwork by @allison_horst"

knitr::include_graphics(here("images", "data_cowboy.png"))

```

The data wrangling process includes data import, tidying, and transformation.  The process directly feeds into the *understanding* or modeling side of data exploration, but in a iterative way.  More generally, data wrangling is the manipulation or combination of datasets for the purpose of analysis, and often you have to rinse and repeat this process.  

```{r out.width = "100%", echo=F, fig.cap = '*Image from "R for Data Science" by Garrett Grolemund and Hadley Wickham.*'}
knitr::include_graphics(here("images", "data-science-wrangle.png"))
```

### Have an Objective

 > **All data wrangling is based on a purpose.**  
 
No one wrangles for the sake of wrangling (usually), so the process always begins by answering the following two questions:

 1. What do my input data look like?
 2. What *should* my output data look like given what I want to do?

### Learn some Core Tools for Common Operations

At the most basic level, going from what your data looks like to what you want it look like will require a few key operations. The more we can learn how to do these operations using common tools, the easier this will be no matter the data that we face. 

<aside> *Tidy data* is an important part of being able to re-use these tools! See this great [blog](https://www.openscapes.org/blog/2020/10/12/tidy-data/) (by Lowndes & Horst) 
</aside>

```{r, echo=FALSE, out.width='80%', fig.cap="Illustrations from the Openscapes blog Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst."}

knitr::include_graphics("images/tidydata_4.jpg")

```

Common operations that occur during data wrangling:

* **Selecting** specific variables
* **Filtering** observations by some criteria
* Adding or modifying (**mutating**) existing variables 
* **Renaming** variables
* **Arranging** rows by a variable
* **Summarizing** a variable conditional on others

The **`{dplyr}`** package provides easy tools for these common data manipulation tasks and is a core package from the [{tidyverse}](https://www.tidyverse.org/) suite of packages. The philosophy of **`{dplyr}`** is that **one** function does **one** thing and the name of the function says what it does. 

### Starting from Scratch

Any reproducible analysis should be easily repeated. Using code or a script allows us to rebuild all the parts, sometimes over and over again. For that reason, it's a good habit to **Restart your R Session** before you kick off a new coding adventure. Sometimes this solves issues, sometimes it's just good to make sure everything runs up to the point you are working. Let's do that now!

 1. Go to **`Session`** > **`Restart R`**!
 2. Check your `Environment` tab...it should be empty!

### Import Libraries and Data 

First we need to load our libraries that we'll be using. Let's use `{here}` and the `{tidyverse}` packages (which includes `{dplyr}` and `{ggplot2}`).

```{r load-libs, eval=FALSE}
library(here)
library(tidyverse)

```

Next we can import a `.csv` of groundwater monitoring stations across California^[see [here for more about this data]()]. 

<aside> We can use either `read.csv` or `read_csv` functions. What's the main difference between the two? Hint, check `class(stations)` for each method.
</aside>

```{r load-data}

# read the stations.csv file and assign it to an object "stations"
stations <- read.csv(here("data", "gwl", "stations.csv"))

```

Below your code that imports the dataset, let's make a new section! Go to **`Code`** > **`Insert Section`**, and type your new section header `Wrangling Data`. We can also use keyboard shortcuts for this (`Ctrl` or `âŒ˜` + `Shift` + `R`). You should notice how this now appears in an expandable table of contents on the right hand side of your script pane (look for the tiny button that has little gray horizontal lines on it). This feature can be very helpful in keeping you and your scripts organized.

## `filter` rows

One of the most common steps to slicing data is **filtering rows** of a dataframe. We can filter by a variable (e.g., column name) or by a condition (i.e., only values greater than 100). Remember, `{dplyr}` is designed so that *one* function does *one* thing, and the name of the function says what it does. 

```{r filterArt, eval=TRUE, echo=FALSE, out.width='80%', fig.cap="Artwork by @allison_horst"}

knitr::include_graphics("images/filter.jpg")

```


Let's filter this large list of potential groundwater station locations (n=`r nrow(stations)` total!) to just stations from Sacramento County. We'll use `filter()`.

```{r dplyr-filt1, echo=TRUE, eval=TRUE}

stations_sac <- filter(stations, COUNTY_NAME == "Sacramento")
nrow(stations_sac)
```

What happened? `filter()` requires the data, and then any number of filtering options. We provided just one. Here the `==` means "equal to", while a single `=` would mean `is`. 

We can also combine filter conditions in the same call! Let's also add another condition that we only want `Residential` wells (a category of the `WELL_USE` column). We can get a count of how many categories of `WELL_USE` have observations (or rows) in the `stations_sac` dataframe using the `table()` function. Then we can combine multiple conditions in our `filter` with a `,` (in `filter`, commas are another way to say `AND`).

```{r dplyr-filt2, echo=TRUE, eval=TRUE}

# get what the categories of WELL_USE are in Sacramento County:
table(stations_sac$WELL_USE)

# combine conditions with "," which is equivalent to AND
stations_sac <- filter(stations, COUNTY_NAME == "Sacramento", WELL_USE == "Residential")
nrow(stations_sac)
```

We can mix and match these `filter` conditions however we choose. What if we want to select multiple counties? The `%in%` is a great way to provide a list of things you want `filter` to use to see if there's a match within your dataframe. Let's filter to 3 counties: Sacramento, Placer, and El Dorado.

```{r dplyr-filt3, echo=TRUE, eval=TRUE}

# filter to multiple counties
stations_multcounties <- filter(stations, COUNTY_NAME %in% 
                                  c("Sacramento", "Placer", "El Dorado"))

```

<aside> To quote or not to quote? Inside tidyverse functions like `filter` we typically **don't** need to quote variable names (columns) but we **do** need to quote specific values.
</aside>

What happened? We used a list (using the `c()` to combine different possible values of `COUNTY_NAME`) to specify what data we wanted to keep. 

### Exclude rows

We can also *exclude* rows based on conditions. To exclude, we can negate something with the `!` symbol in front of the condition we want to use. Let's exclude all stations from Yolo County. Here we show 3 ways to to do the same thing!

```{r dplyr-filt4, echo=TRUE, eval=TRUE}

# include everything except Yolo County...3 different ways
stations_trim <- filter(stations, !COUNTY_NAME %in% c("Yolo"))
stations_trim <- filter(stations, !COUNTY_NAME == "Yolo")
stations_trim <- filter(stations, COUNTY_NAME != "Yolo")

```

## `select` columns

We can also select specify columns or variables of interest using the `select()` function. Similar to `filter`, we pass the data, and the conditions we want to use subset our data. Let's select only `STN_ID`, `LATITUDE`, `LONGITUDE`, and `COUNTY_NAME`.


```{r dplyr-sel1, echo=TRUE, eval=TRUE}

# select specific columns to keep
stations_sel1 <- select(stations, c(STN_ID, LATITUDE, LONGITUDE, COUNTY_NAME))
names(stations_sel1) # names of columns
```

Remember we don't have to quote the variable names here! What if we want to just remove a column? We can use the same structure, but put a "**`-`**" in front of our list or condition.

```{r dplyr-sel2, echo=TRUE, eval=TRUE}

# select columns to drop
stations_sel2 <- select(stations, -c(LATITUDE:BASIN_NAME, COUNTY_NAME))
ncol(stations_sel2) # how many columns?
```

Here we used the `:` to say columns from `LATITUDE` *through* `BASIN_NAME` *and* `COUNTY_NAME`.

There are some very powerful options that exist within `select()`, including the ability to rename columns, or select by things that relate to the column names. Let's select columns that start with "W" or contain "NAME".

```{r dplyr-sel3, echo=TRUE, eval=TRUE}

# select columns to drop
stations_sel3 <- select(stations, starts_with("W"), contains("NAME"))
names(stations_sel2)
```

## The Pipe ( `%>%` ) 

We now have two ways to slice and dice our data, filter rows and select by columns. A very useful tool you may have seen is something called a "pipe". In R, this is represented by this symbol: **`%>%`**

The pipe is a way to chain multiple commands together, and it passes the result of the first command to the second command. This can be particularly helpful in R when you don't need to (or want to) assign temporary steps to your code. For example, let's filter, select, and rename a column without the `%>%`, and then do it with the `%>%`.

```{r dplyr-nopipe, echo=TRUE}

# filter
stations_multcounty1 <- filter(stations, COUNTY_NAME %in% 
                                  c("Sacramento", "Placer"))

# select
stations_multcounty2 <- select(stations_multcounty1, starts_with("W"), 
                               contains("NAME"), contains("ID"))

# rename the STN_ID to station_id: rename(new_col_name, old_col_name)
stations_multcounty3 <- rename(stations_multcounty2, station_id=STN_ID)

```

Now we can try with the `%>%`. Notice we don't need to explicitly define the data set for each step, because it is being "piped" from the previous step.

```{r dplyr-pipe, echo=TRUE}

# filter, select, and rename
stations_multcounty <- stations %>%
  filter(COUNTY_NAME %in% c("Sacramento", "Placer")) %>% 
  select(starts_with("W"), contains("NAME"), contains("ID")) %>%
  rename(station_id=STN_ID)

```

We piped (`%>%`) the data to `filter()`, and the result of the `filter()` to `select()`, and then piped the result of that to `rename()`. Since we assigned all of this at the very beginning to `stations_multcounty`, that's our final output. We could also use a reverse assign ` -> ` and assign our object at the end of the pipes, it really is up to you.


:::challenge

<font color="#009E73">**Challenge 1: You Try!**</font> 

1. Using the `stations` dataset, filter to only **`Residential`** wells that have a `WELL_DEPTH > 1000` feet.  
2. Select only `STN_ID`, `WELL_DEPTH`, `WELL_NAME`, `BASIN_NAME`, and `COUNTY_NAME` columns in the dataframe. 
3. How many records are in Los Angeles?   

:::

<br>

<details>
  <summary class="challenge-ans-title"><font color="#0072B2">**Click for Answers!**</font></summary>
  <div class="challenge-ans-body">

```{r challenge-1}
# filter
challenge1 <- stations %>% 
  filter(WELL_USE=="Residential", WELL_DEPTH > 1000) %>% 
  select(STN_ID, WELL_DEPTH, ends_with("NAME"))
names(challenge1)

table(challenge1$COUNTY_NAME)
  

```

  </div>
</details>

## `mutate` existing data

`mutate` is a function that modifies existing data, either by adding a new column, or modifying existing columns. When using `mutate` we expect R to return some modified version of our input dataframe... but it should still be a version of the original dataframe.

```{r mutateFig, out.width='70%', echo=FALSE, fig.cap="Artwork by @allison_horst"}

knitr::include_graphics("images/mutate.png")

```


`mutate` allows us to pass other functions or data into our dataframe, and is a powerful way to clean and tidy our data.

Let's create a new `WELL_DEPTH_m` column which converts the existing `WELL_DEPTH` column to meters. We pass the new column name and then the function or operation we want to peform to make the new column (or modify an existing column).

```{r mutate1, eval=TRUE, echo=TRUE}

stations_mutate1 <- stations %>% 
  mutate(WELL_DEPTH_m = WELL_DEPTH * 0.3048)

```

Let's use `ggplot2` to check and see if this worked? We can use the `%>%` here too!

```{r mut1ggplot1, eval=TRUE, echo=TRUE, warning=FALSE}

stations_mutate1 %>% ggplot() + 
  # this in feet
  geom_point(aes(x=STN_ID, y=WELL_DEPTH), color="cyan4", alpha=0.5) +
  # this in meters
  geom_point(aes(x=STN_ID, y=WELL_DEPTH_m), color="maroon", pch=21, alpha=0.8)

```

Great, this looks about right! It is rarely a bad idea to "visualize" your outcomes whenever possible, these are great ways to double check things are happening as you intend when data wrangling. 

<details>
  <summary class="extra-practice-title">Extra Practice</summary>
  <div class="extra-practice-body">
Let's pause a moment and do this *without* the tidyverse so we can have a chance to see both methods. There are many positives about knowing different approaches, and also when troubleshooting, it can be helpful to understand different ways of doing the same thing. Can you spot a few differences?

```{r baseR-mutate, echo=TRUE, eval=FALSE}

# filter and select (subset)
stations_basefilter <- stations[stations$WELL_USE=="Residential" & 
                                  stations$WELL_DEPTH > 1000,
                                c(1, 4, grep("NAME",colnames(stations)))] 

# mutate
stations_base1 <- stations # make a copy so we don't alter original data
stations_base1$WELL_DEPTH_m <- stations_base1$WELL_DEPTH * 0.3048

# ggplot
ggplot(data=stations_base1) + 
  geom_point(aes(x=STN_ID, y=WELL_DEPTH), color="cyan4", alpha=0.5) +
  # this in meters
  geom_point(aes(x=STN_ID, y=WELL_DEPTH_m), color="maroon", pch=21, alpha=0.8)

```

  </div>
</details>

## `group_by` & `summarize`

The final core `{dplyr}` "verbs" we want to be sure folks are aware of, are `group_by` and `summarize`. These are really useful to transform your data into an entirely **new** dataset, and to do many operations based on different columns (variables) within your dataset.

For example let's say we want to get a count of how many records exist for each county. We can use **`group_by`** and **`count`**.

```{r grpby1, eval=T, echo=T}

cnt_by_county <- stations %>% 
  group_by(COUNTY_NAME) %>% count()

head(cnt_by_county)
```

What happened? `group_by()` will group the data based on whatever variable or variables you include within that function. Once our `dataframe` is grouped, there are many options and functions we can apply, including things like `count()`, `tally()`, and `summarize()`.

Let's look at this a bit more. What if we really only want the 5 counties with the most observations? We can add two additional options here. The `arrange()` function will sort our data by a variable(s) of our choice, and we can use `desc()` to specify we want that data in descending order. We then pipe that to `head()` and ask for just the first 5 rows.

```{r grpby2, eval=T, echo=T}

stations %>% 
  group_by(COUNTY_NAME) %>% count() %>% # count by COUNTY_NAME
  arrange(desc(n)) %>% # sort by column n in descending order
  head(5) # return the first 5 rows

```

### Summarizing Data

Similar to `mutate`, `summarize` allows us to create new columns. The difference is `summarize` creates a completely new dataframe based on the `group_by`. So we only use `summarize` following a `group_by`. Here we can do a similar analysis to what we did above, but now we can customize our columns and what we want to calculate. We'll also plot the top 10 records and add a title.

```{r grpby3, eval=T, echo=T}

stations %>% 
  group_by(COUNTY_NAME) %>% 
  summarize(total_records = n()) %>% 
  arrange(desc(total_records)) %>% # need to specify the column name "total_records"
  head(10) %>% # return the first 10 rows 
  ggplot() + geom_col(aes(x=COUNTY_NAME, y=total_records)) +
  labs(title="Groundwater Stations in CA", 
       subtitle="Top 10 Counties")

```

<aside> This is a plot to consider rotating the axis labels. How do we do that? Practice your web-search skills or visit the [ggplot2 help page](https://ggplot2.tidyverse.org/index.html). *Hint: look at `theme(axis.text.x = )`*
</aside>

We can add additional groups or columns depending on what we are interested in. Let's try the following:

 - find the mean well depth by each `WELL_USE` category using **`summarize`**
 - **`filter`** to only groups that have more than 10 stations and no NA's (*for NA's, we can use the `is.na()` function, and the negate or `!` symbol we used previously*)
 - Let's pipe this to ggplot and make a boxplot to visualize the result

```{r grpby4, eval=T, echo=T}

stations %>% 
  group_by(COUNTY_NAME, WELL_USE) %>% 
  summarize(mean_well_depth = mean(WELL_DEPTH),
            total_records = n()) %>% 
  filter(total_records > 10, !is.na(mean_well_depth)) %>% 
  ggplot() + 
  geom_boxplot(aes(x=WELL_USE, y=mean_well_depth), fill="seagreen", alpha=0.5) +
  labs(title="Mean Well Depth for Groundwater Stations in CA",
       subtitle="For groups with >10 stations",
       x="Well Use", y="Mean Well Depth (ft)")

```

 What happened? We grouped by two different variables, and then applied functions (or transformed the data) to those groups using `summarize`, then we filtered, and visualized. The order of these steps can influence your analysis and result, so it's good to consider when you want to filter and when you want to group_by/summarize things. Let's try a challenge!
 
:::challenge

<font color="#009E73">**Challenge 2: You Try!**</font> 

1. Using the code we ran above, figure out how to make this boxplot without the "`Unknown`" and blank (`""`) `WELL_USE` categories. 
2. Try adding points as another layer over your boxplot using `geom_jitter`.
3. For a bonus challenge, try adding a caption with information about this dataset, and plotting these data in meters instead of feet.

:::

<br>

<details>
  <summary class="challenge-ans-title"><font color="#0072B2">**Click for Answers!**</font></summary>
  <div class="challenge-ans-body">

```{r challenge-2}

stations %>% 
  filter(WELL_USE!="", WELL_USE!="Unknown") %>% 
  group_by(COUNTY_NAME, WELL_USE) %>% 
  summarize(mean_well_depth = mean(WELL_DEPTH),
            mean_well_depth_m = mean_well_depth * 0.3048,
            total_records = n()) %>% 
  filter(total_records > 10, !is.na(mean_well_depth_m)) %>%
  ggplot() + 
  geom_boxplot(aes(x=WELL_USE, y=mean_well_depth_m), fill="seagreen", alpha=0.5) +
  # add jittered points
  geom_jitter(aes(x=WELL_USE, y=mean_well_depth_m), color="forestgreen", alpha=0.5) +
  # add labels and caption
  labs(title="Mean Well Depth for Groundwater Stations in CA",
       subtitle="For groups with >10 stations",
       caption = "These are Groundwater Station data from...",
       x="Well Use", y="Mean Well Depth (ft)")

```

  </div>
</details>

### Remember to `ungroup`

A common blunder that can impact downstream analyses, or even the current analysis you may want to do is forgetting to ungroup. `{dplyr}` is smart, and will do exactly as you program it to do, but when you use `group_by`, your output dataframe **will retain those groups** for future use. This can be helpful in some cases, but it's important to keep an eye on and be aware of (for when it's *not* helpful). We can always use `class()` to better check this. Let's try some code from above and check with `class()`.

```{r ungroup, eval=T, echo=T}

stations %>% 
  filter(WELL_USE!="", WELL_USE!="Unknown") %>% 
  group_by(COUNTY_NAME, WELL_USE) %>% 
  summarize(mean_well_depth = mean(WELL_DEPTH),
            total_records = n()) %>% 
  filter(total_records > 10, !is.na(mean_well_depth)) -> stations_by_cnty_use

class(stations_by_cnty_use)

```

Here we can see the `grouped_df` title, which tells us this is a **grouped** dataset. We can easily *ungroup* with the concisely named `ungroup()` function. This isn't required, but it's a good habit to get into (or be aware of).

```{r ungroup2, eval=T, eval=T}

stations_by_cnty_use <- ungroup(stations_by_cnty_use)
class(stations_by_cnty_use)
```

Congratulations! You've made it through `{dplyr}`. There are many more great options and tricks you can learn about `{dplyr}`, but hopefully this will get you going.
