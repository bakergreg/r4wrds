{
  "articles": [
    {
      "path": "index.html",
      "title": "Welcome!",
      "description": "Increasing your efficiency with reproducible workflows in R\n",
      "author": [],
      "contents": "\n\n  .title{\n    display: none;\n  }\n\nWho is this course for?\nWhat makes an Intermediate R user? This course is most relevant and targeted at folks who:\nTook the Introductory R4WRDS course\nRegularly use R and want improve their efficiency and skill set\nHave a general understanding and proficiency in using {dplyr}, {ggplot2}, {sf}, and {rmarkdown}\nUnderstand (and use) general best practices for data science in R\nWhat will you learn?\nIn this course, we will move more quickly, assume familiarity with basic R skills, and also assume that the participant has experience with more complex workflows, operations, and code-bases. Each module in this course functions as a “stand-alone” lesson, and can be read linearly, or out of order according to your needs and interests. Each module doesn’t necessarily require familiarity with the previous module.\nThis course emphasizes:\nIntermediate scripting skills like iteration, functional programming, writing functions, and controlling project workflows for better reproducibility and efficiency.\nApproaches to working with more complex data structures like lists and timeseries data.\nThe fundamentals of building Shiny Apps.\nPulling water resources data from APIs.\nIntermediate mapmaking and spatial data processing.\nIntegrating version control in projects with git.\n\n\n\nFigure 1: Artwork by @allison_horst\n\n\n\n\nCourse Modules\nIn this course, we will:\nCheck/Update R/RStudio and Packages\nVersion Control with git\nProject Management and workflows\nInteractive visualization\nSimple shiny\nIteration with complex data\nParameterized reports\nAdvanced spatial R and mapmaking\nPulling data from APIs\n\n\nWhy R?\nR is an open-source language for statistical computing and a general purpose programming language. It is one of the primary languages used for data science, modeling, and visualization.\n\nWorkshop Overview\nWe will follow the SFS Code of Conduct throughout our workshop.\n\nSource content\nAll source materials for this website can be accessed at [final GH location, perhaps CAOpenWater/wrds].\n\nAttribution\nContent in these lessons has been modified and/or adapted from Data Carpentry: R for data analysis and visualization of Ecological Data, the USGS-R training curriculum here, the NCEAS Open Science for Synthesis workshop here, Mapping in R, and the wonderful text R for data science.\n\n\nNext module: Project Management\nsite last updated: 2021-04-29 12:34\n\n\n\n",
      "last_modified": "2021-05-16T13:12:05-07:00"
    },
    {
      "path": "m_iteration.html",
      "title": "From loops to functions",
      "description": "Multiple ways to iterate in `R`\n",
      "author": [],
      "contents": "\n\nContents\nMany paths to the same summit\nfor loops\nlapply()\nmap()\nAdditional Resources\n\n\nLearning objectives\nUnderstand the costs and benefits of loops and functional programming\nPractice iteration with for loops\nPractice iteration with lapply() and the {purrr} family of functions\nUnderstand how interact with the list object type\n\n\nMany paths to the same summit\nOne of the wonderful things about R is that there are often many ways to achieve the same result. This makes R expressive, and as you practice and expand your R vocabulary, you’ll inevitably find multiple ways to obtain the same results. Some ways may be long and meandering, and others may be more direct and easy to remember. In this module we will practice two approaches towards iteration—the art of not repeating yourself—specifically, for loops1 and functions (functional programming).\nFor most parallel problems (i.e., the current operation does not depend on the one that came before), functional programming offers significant advantages to for loops. However, both loops and functions are powerful ways to automate processes and workflows. We will review how and when to use each, and their pros and cons.\nThe core objective of this lesson is to practice harnessing the power of iteration in your workflows, and to demonstrate ways to iterate with functional programming. If you can define a function to work with one object in R, functional programming allows you to scale that function to any number of objects (assuming you have enough computing resources). Together, functional programming and iteration allow automation at scale, and put R a cut above GUI-based data workflows that require “clicking though” an analysis.\nProficiency in functional programming means you can turn an entire workflow into a function, or individual steps for a workflow can each be discrete functions, making it easy to reproduce or revise your work.\nProficiency in iteration means you can apply developed functions many times with little effort, which allows you to work with small or large datasets with ease.\nProficiency in automation means you can remove yourself from running the code entirely so that it happens all on its own—we will demonstrate approaches to automation in the next module.\nIn this module, let’s imagine you’re a data manager, and need to provide data to a group of users. We’ll practice iteration on reading and writing data to illustrate a transition from for loops to functional programming.\n\nfor loops\n\n“Don’t repeat yourself. It’s not only repetitive, it’s redundant, and people have heard it before.” -Lemony Snicket\n\nThe for loop is ubiquitous across programming languages and a fundamental concept that allows us to obey a core concept in programming: don’t repeat yourself. The for loop is a good place to begin a discussion of iteration because it makes iteration very explicit—you can see exactly what is taking place in each iteration, or loop.\nA common problem we might face is reading multiple data frames into R. In the /data/gwl folder, we have station data for El Dorado, Placer, and Sacramento counties. We can read these in one by one by copy and pasting code, but we’re repeating ourselves. This may not matter for only 3 counties, but if we were to use all 58 counties with groundwater level data, this would be a non-scaleable approach prone to human error.\n\n\nlibrary(tidyverse)\nlibrary(here)\n\neldorado <- read_csv(here(\"data\", \"gwl\", \"county\", \"El Dorado.csv\"))\nplacer   <- read_csv(here(\"data\", \"gwl\", \"county\", \"Placer.csv\"))\nsac      <- read_csv(here(\"data\", \"gwl\", \"county\", \"Sacramento.csv\"))\n\n\n\nWe can replace this code with a for loop and read all of these items into a list2.\n\n\n# list all files we want to read in\nfiles_in <- list.files(here(\"data\", \"gwl\", \"county\"), full.names = TRUE)\n\n# initialize a list of defined length\nl <- vector(\"list\", length = length(files_in))\n\n# loop over all files and read them into each element of the list\nfor(i in seq_along(l)){\n  l[[i]] <- read_csv(files_in[i])\n}\n\n\n\nAfter the loop finished, if you run i in the console, you’ll notice that it has a value of 3. That’s because i is updated as the loop evaluates, and 3 is the last iteration of i in the loop. This can be useful when debugging a loop because the last index that the loop was evaluated may identify the location of a problem that breaks the loop.\nWhat happened above is that we evaluated the loop first starting with 1 in the place of i, and went to length(l), which is 3, each time placing the integer wherever there is an i in the above loop. Although our index is i, we can use any other unquoted character string, like j, k, or even index — its only purpose is to hold the index that we iterate through.\nAfter the loop evaluates, we can access each element of the list with double bracket [[ notation, subsetting either by an integer index, or a name if one exists. This list doesn’t have names, but we could set them by assigning a vector of names to names(l).\n\n\n# access first list element - El Dorado county dataframe\nl[[1]] \n\n\n\nfor loops happen sequentially, and require us to think of code in terms of objects that we iterate through, index by index. This can result in both slower and duplicated (verbose) code. In addition to writing efficient code, a core rule of good programming is to not repeat oneself.\nLet’s imagine now that you needed to write each of these data into a separate file, separated by the unique ID of each station (e.g., the SITE_CODE). Your entire loop would look like this:\n\n\n# initialize a list of defined length\nl <- vector(\"list\", length = length(files_in))\n\n# loop over all files and read them into each element of the list\nfor(i in seq_along(l)){\n  l[[i]] <- read_csv(files_in[i])\n}\n\n# combine all list elements into a single dataframe\n# then split into another list by SITE_CODE.\nldf <- bind_rows(l)\nldf <- split(ldf, ldf$SITE_CODE)\n\n# loop over each list element and write a csv file\ndir.create(here(\"data\", \"gwl\", \"site_code\"))\n# here we make a list of files names ( names(ldf) is a vector )\nfiles_out <- glue::glue(\"{here('data', 'gwl', 'site_code', names(ldf))}.csv\")\n\nfor(i in seq_along(ldf)){\n  write_csv(ldf[[i]], files_out[i])\n}\n\n\n\ndplyr::bind_rows(l) row binds a list of data.frames similar to base R’s do.call(rbind.data.frame, l), but is more strict, and will throw an error if column types don’t match.\n\n\n\nThis is a lot of code to accomplish a relatively standard iterative workflow of reading in a directory of files, combining them, and writing them out. Functional programming can simplify loop-based workflows, run faster, and encourage you to think conceptually about the transformations at play, without worrying about tracking a changing index. Moreover, loops in R are usually unnecessary unless the result of the ith index depends on the i-1 (previous) index.\nlapply()\nBase R gives us a toolkit for functional programming via the apply family of functions, specifically lapply() and mapply(). The “l” in lapply() stands for “list”, and can be read as “list apply”. The apply functions are designed to iterate over lists, matrices, rows, or columns, and are very flexible. For example, we can simplify the for loops above as:\n\n\n# read\nl <- lapply(files_in, read_csv)\n\n# bind and split by SITE_CODE\nldf <- bind_rows(l)\nldf <- split(ldf, ldf$SITE_CODE)\n\n# write out: requires function (write_csv) \n# with 2 args (x = ldf & y = files_out)\nmapply(function(x, y) write_csv(x, y), ldf, files_out) \n\n\n\nWe can make this clearer by extracting the anonymous function3 above, assigning it to an identifier, and calling that in mapply():\n\n\nmy_function <- function(x, y){ \n  write_csv(x, y)\n}\n\nmapply(my_function, ldf, files_out) \n\n\n\nNotice that we don’t need to initialize a list to store output, or keep track of indices. The emphasis is on the transformation taking place, not index bookkeeping, and setting up looping patterns. The result is the same, and we have a much clearer way to approach the problem.\n\nmap()\nThe map family of functions in the {purrr} package improves on base R’s apply functions with simplified syntax, type-specific output that makes it harder to accidentally create errors, and convenience functions for common operations. When we combine map() with pipes (%>%) we can greatly simplify our code.\nFirst, to mimic what we did with lapply() above:\n\n\n# read\nl <- map(files_in, ~read_csv(.x))\n\n# bind and split by SITE_CODE\nldf <- bind_rows(l)\nldf <- group_split(ldf, SITE_CODE)\n\n# write\nwalk2(ldf, files_out, ~write_csv(.x, .y)) \n\n\n\nThe .x signifies each of the individual elements of passed into the function, and can be thought of as a placeholder for the input. We begin read_csv() with a ~ to indicate the beginning of a function.\nWe can further simplify our code with map_df() to automatically row bind the list elements into one dataframe. Moreover, we can pipe these statements together to avoid creating the intermediate ldf object.\n\n\n# read and bind, split, and write\nmap_df(files_in, ~read_csv(.x)) %>% \n  group_split(SITE_CODE) %>% \n  walk2(files_out, ~write_csv(.x, .y))\n\n\n\nLet’s break down what we did above.\nWe mapped the function read_csv() over the vector of file paths, files_in. Although this function is simple, in practice we can map a large and complex function in the same way. The _df in map_df() means we pass the list otherwise returned by map() into bind_rows(), and thus return one combined dataframe of all the csv files we read instead of a list of dataframes.\nWe used group_split() to split the combined dataframe by SITE_CODE which returns a list of dataframes ordered by the unique values of the grouping variable. This is identical to base R’s split() except it doesn’t return a named list.\nWe walk2()ed over the list of dataframes (one for each SITE_CODE), and the files_out vector from above, and wrote a csv for each pair of objects (.x = dataframe and .y = output file path).\n\nExtra information\n\nWhat would happen if we used map2() instead of walk2() in the code above?\n\nClick for the Answer!\n\nwalk2() is a special case of walk() which takes 2 vector inputs instead of 1. The first and second vector inputs are called in the function with .x and .y like so: walk2(input_1, input_2, ~function(.x, .y)). We use walk() instead of map() whenever we want the side effect of the function, like writing a file. We could also use map() here, but it would unnecessarily print each of the 723 dataframes.\nYou may now we wondering if there is a map3(), map4() and so on. To map over more than 2 inputs at once, check out the “parallel” map purrr::pmap() function, which is similar to base R’s mapply().\n\n\n\nYou may notice that we used an intermediate object (files_out) from above. We could re-write our chain without this object by creating it within the function call:\n\n\n# read, bind, and write\nmap_df(files_in, ~read_csv(.x)) %>% \n  group_split(SITE_CODE) %>% \n  walk(\n    ~write_csv(\n      .x, \n      paste0(here(\"data\",\"gwl\",\"site_code\"), \"/\", .x$SITE_CODE[1], \".csv\")\n    )\n  )\n\n\n\nThe ~, .x, and .y syntax may seem confusing at first, but with some practice it will become easier, and it provides a consistent syntax to express complex ideas about your code. More more importantly, the emphasis is on keeping track of functions rather than creating and managing the scaffolding of for loops.\nTaking things one step further, imagine you were:\nonly interested in WELL_USE types equal to “Observation”\nyou needed to convert the WELL_DEPTH from feet to meters\nyou needed to output the data as three shapefiles, one for each county\nWe could capture these transform steps in a function, store it in our /functions folder as described in the project management module, and in this way, clean up our workspace so we can keep track of the functions applied on our data and keep our scripts short and readable.\nStore this in a file /functions/f_import_clean.R:\n\n\n# import dataframe, filter to observation wells, convert well \n# depth feet to meters, project to epgs 3310, & export the data\nf_import_clean_export <- function(file_in, file_out){\n  read_csv(file_in) %>% \n    filter(WELL_USE == \"Observation\") %>% \n    mutate(well_depth_m = WELL_DEPTH * 0.3048) %>% \n    sf::st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4269) %>% \n    sf::st_transform(3310) %>% \n    sf::st_write(file_out, delete_layer = TRUE)\n}\n\n\n\nNow we can walk() inputs over this function with ease in our main script without keeping track of loops and indices, or even the function internals, which are neatly stored in their own file.\n\n\nsource(here(\"functions\", \"f_import_clean_export.R\"))\n\n# create a directory to store results\ndir.create(here(\"results\"))\n\n# vectors with function args: input (.x) & output (.y) files\nfiles_in  <- list.files(here(\"data\", \"gwl\", \"county\"), full.names = TRUE)\nfiles_out <- here(\"results\", str_replace_all(basename(files_in), \".csv\", \".shp\"))\n\nwalk2(files_in, files_out, ~f_import_clean_export(.x, .y))\n\n\n\nNow we check the /results directory, to verify it is populated with the shapefiles we just wrote.\n\nAdditional Resources\nIteration is a core skill that will allow you to scale your workflow from small to large while maintaining reproducibility. Combined with a proficiency in functional programming, you will more easily develop and store functions, declutter your workspace to focus on important transformations, streamline tracking down and fixing bugs, and keep track of your data pipelines. Your ability to perform and re-perform arbitrarily complex workflows will exponentially increase.\nWe recommend the following places to start to learn more about functional programming in R:\nR for Data Science book section on Programming\nA talk about functional programming by Hadley Wickham\nAdvanced R book chapter on Functional programming\n\nLesson adapted from R for Data Science.\n\n\nPrevious module:Simple Shiny Next module:Reproducible Workflows\n\nFor loops are an example of imperative programming, which differs from functional programming in that it emphasizes the steps to take to change the state of a computer rather than composing and applying functions.↩︎\nFor a review of R’s list data structure, see the data structures module and Hadley Wickham’s excellent “R for Data Science” chapter on vectors.↩︎\nAn anonymous function is a function that is not assigned to an identifier. In other words, it is created and used but doesn’t exist in the Gobal Environment as a function you can call by name. The benefit of using anonymous functions is that they allow you to quickly write single-use functions without storing and managing them. For example, in the expression lapply(1:5, function(x) print(x)), the anonymous function is function(x) print(x). To make it a regular, named function, we assign it to a value, like print_value <- function(x) print(x). Then it can be called by name like so: lapply(1:5, print_value).↩︎\n",
      "last_modified": "2021-05-16T12:19:22-07:00"
    },
    {
      "path": "m_project_management.html",
      "title": "Project Management & Data Organization",
      "description": "Approaches towards organization and efficiency.\n",
      "author": [],
      "contents": "\n\nContents\nFirst principles\n.RProfile\nGlobal (user-level) .Rprofile\nLocal (project-level) .Rprofile\nUsing .Rprofile\n\n.Renviron\nStrategies to organize projects/code\nAbstracting Functions from Code\n\n{renv}\n\n\nLearning objectives\nImplement best practices for reproducible data science\nCreate and use an RStudio project (.RProj)\nUnderstand filepaths and {here}\nUnderstand how and when to modify .RProfile and .Renviron files\nApply strategies to organize functions and scripts\nUnderstand package environments and how to manage them with {renv}\n\nFirst principles\nThe first step in any data science project is to set up and maintain a clean, predictable development environment. As you accumulate raw data, write code, and generate results, things can get messy if you don’t stick to good programming naming and organization habits. In this module we’ll cover how to keep your projects organized and consistent, which ultimately will make projects more reproducible and keep your workflow more efficient and fun.\nA development environment is the set of tools you use to process data and the toolshed. R and packages are your tools, and an RProject is your toolshed.\n\nREVIEW\n\nAlthough this is an intermediate level course, we will revisit introductory material on “Project Management” because no matter your skill level in R, strategic project management remains fundamental. Subsequent modules in this course assume familiarity with {here}, .Rprojects, naming conventions, and general best practices.\n\n\n\nAfter reviewing core introductory topics, we will discuss .RProfile and .Renviron files and when to use them. To review core best practices we recommend particular familiarity with the following concepts:\nFilenaming\nRelative filepaths and the {here} package\nBest practices for project organization\n.RProfile\nIf you have user- or project-level code that needs to be run every time you start up R, customizing your .RProfile can streamline this operation.\nThe .RProfile file is an actual (hidden) file that is automatically sourced (run) as R code when you open an R session. A .RProfile file can live in the project root directory, or the user’s home directory, although only one .RProfile can be loaded per R session or RProject. If a project-level .Rprofile exists, it supersedes the user-level .Rprofile.\nYou can also “show hidden files” (instructions for Mac and PC) and edit .RProfile in a text editor. You can also do so in R with file.edit(\"~/.RProfile\").\nGlobal (user-level) .Rprofile\nThe easiest and most consistent way to edit your .RProfile file across operating systems is with the {usethis} package. Run usethis::edit_r_profile() to open your user-level (or global) .RProfile. This is the default .Rprofile that will be used for any and all projects unless you have a local (project-level) .Rprofile.\n\n\nusethis::edit_r_profile()\n\n\n\nLocal (project-level) .Rprofile\nWe can create or edit a local or project-level .RProfile with the scope argument inside the edit_r_profile function. Remember, a project-level .RProfile will supersede the global .Rprofile.\n\n\nusethis::edit_r_profile(scope = \"project\")\n\n\n\nUsing .Rprofile\nTo illustrate how .RProfile works, let’s do something cool and useless. We’ll write a short program that greets us with a random inspirational quote, and then we’ll put in .RProfile so it runs whenever we start up R.\nThe {cowsay} package is a fun way to print text animal art.\n\n\ncowsay::say(what = \"hello world!\", by = \"cow\")\n\n\n\n ----- \nhello world! \n ------ \n    \\   ^__^ \n     \\  (oo)\\ ________ \n        (__)\\         )\\ /\\ \n             ||------w|\n             ||      ||\n\nLet’s randomize the animal displayed and make the message it says one of the motivational quotes found at this Github repo, copy and paste the code into our .RProfile, and restart R.\n\n\nlibrary(cowsay) # animals!\nlibrary(glue)   # pasting things together\n\n# get vector of all animals\nanimals <- names(cowsay::animals)\n\n# get pieces to make link\nrepo <- \"JakubPetriska/060958fd744ca34f099e947cd080b540\"\ncsv <- \"raw/963b5a9355f04741239407320ac973a6096cd7b6/quotes.csv\"\n\n# get dataframe of inspirational quotes\nquotes  <- readr::read_csv(glue(\"https://gist.githubusercontent.com/{repo}/{csv}\"))  \n\n# make full quote\nquotes$full_quote  <- glue(\"{quotes$Quote} - {quotes$Author}\")\n\n# now use it!\ncowsay::say(sample(quotes$full_quote, 1), by = sample(animals, 1))\n\n\n\n ----- \nYou can tell whether a man is clever by his answers. You can tell whether a man is wise by his questions. - Naguib Mahfouz \n ------ \n    \\   \n     \\  \n      \\\n       \\   _,\n      -==<' `\n          ) /\n         / (_.\n        |  ,-,`\\\n         \\\\   \\ \\\n          `\\,  \\ \\\n           ||\\  \\`|,\n jgs      _|| `=`-'\n         ~~`~`\n\nrm(animals, quotes) # remove the objects we just created\n\n\n\n.Renviron\nSometimes you need to store sensitive information, like API Keys, Database passwords, data storage paths, or general variables used across all scripts. We don’t want to accidentally share these information, accidentally push them to Github, or copy and paste them over and over again from script to script. We also might want to build a codebase that relies on a few variables that another user can set in their own system in a way that works for them. Environmental variables are the way to address all of these concerns.\nEnvironmental variables are objects that store character strings. They are accessible from within R upon startup. To view all environmental variables, use Sys.getenv(). You can also pull out one environmental variable at a time by passing in its name, for instance:\n\n\nSys.getenv(\"USER\")\n\n\n[1] \"richpauloo\"\n\nYou can set your own environmental variables which are stored in another hidden file called .Renviron (this is the Python analog of .env). Keep in mind, .Renviron files typically contain lists of environmental variables that look similar to R code but it is actually not running R code…so don’t put R code in your .Renviron file! If we need to run R code when starting up R, we use .RProfile.\nTo illustrate the use of .Renviron, we run usethis::edit_r_environ(), add the environmental variable ANIMAL = \"cat\", save, and restart R.\n\n\nusethis::edit_r_environ()\n\n\n\nWe can access our environmental variable as follows (remember you need to restart R for changes to take effect, try Session > Restart R):\n\n\nSys.getenv(\"ANIMAL\")\n\n\n\nWe can use our environmental variable, for instance, in a function.\n\n\ninspire_me <- function(animal){\n\n  # get pieces to make link\n  repo <- \"JakubPetriska/060958fd744ca34f099e947cd080b540\"\n  csv  <- \"raw/963b5a9355f04741239407320ac973a6096cd7b6/quotes.csv\"\n  \n  # silently read dataframe\n  suppressMessages(\n    quotes  <- readr::read_csv(\n      glue::glue(\"https://gist.githubusercontent.com/{repo}/{csv}\")\n    )  \n  )\n  \n  # paste together the full quote\n  quotes$full_quote  <- paste0(quotes$Quote, \" -\", quotes$Author)\n  \n  # make a user-specified animal say the quote\n  cowsay::say(sample(quotes$full_quote, 1), by = animal)\n\n}\n\n# have the environmental variable say a quote\ninspire_me(Sys.getenv(\"ANIMAL\"))\n\n\n\nAlthough it may not appear powerful in this trivial example, when a project grows substantially large and complex, or when managing multiple sensitive passwords and access tokens, environmental variables are a standard approach that are widely used.\nImportant note, both these files .Renviron and .Rprofile need to end with a blank newline at the end. If this isn’t present, the last line of the file is ignored, and there isn’t a message or error associated with this. The usethis functions typically take care of this for you, but be aware of it just in case!\n\nPause and think\n\nIn the example function above, we might notice that reading in a url from a csv every time we run inspire_me() is a lot of unnecessary overhead. Where else might we be able to read that csv in automatically when R starts up, so that it’s available for our inspire_me() function, and that we only need to read it once?\n\n\n\nClick for Answers!\n\nWe can move read step of the csv into a project-level RProfile, so it’s available to the project where we need this csv, but not to any general R session we may open outside of the project.\n.RProfile\n\n\n# get pieces to make link\nrepo <- \"JakubPetriska/060958fd744ca34f099e947cd080b540\"\ncsv  <- \"raw/963b5a9355f04741239407320ac973a6096cd7b6/quotes.csv\"\n\n# silently read dataframe\nsuppressMessages(\n  quotes  <- readr::read_csv(\n    glue::glue(\"https://gist.githubusercontent.com/{repo}/{csv}\")\n  )  \n)\n\n# paste together the full quote\nquotes$full_quote  <- paste0(quotes$Quote, \" -\", quotes$Author)\n\n\n\nModified function\n\n\ninspire_me <- function(animal){\n\n  # make a user-specified animal say the quote\n  cowsay::say(sample(quotes$full_quote, 1), by = animal)\n\n}\n\n\n\n\nStrategies to organize projects/code\nBest practices for writing code across languages typically recommend package imports and function definitions at the top of a script, followed by code. For example, a script may look like this:\n\n\n# import packages\nlibrary(package_1)\nlibrary(package_2)\n\n# define functions\nmy_first_function <- function(){\n  print(\"hello\")\n}\n\nmy_second_function <- function(){\n  print(\"world\")\n}\n\n# run code/functions\nmy_first_function()\nmy_second_function()\n\n\n\nThese approaches work well when scripts are relatively simple, but as a project grows large and complex, it’s best practice to move functions into another script or set of scripts, and break up your workflow into discrete steps.\nFor instance, although the inspire_me() function above is relatively simple, we can pretend that the read, transform, and print steps carried out in the function were themselves long functions in part of a much more complex, real-world workflow. Imagine we created a script called functions.R that contained the following code. Don’t worry if you haven’t seen purrr::walk() before. We’ll cover this in a later module on iteration, and all you need to know about it now is that it “walks” over each input and applies a function. In this case, we apply the require() function to a vector of package names to load them.\n\nNotice that all functions start with f_. This prefix makes it easy to read in a script and takes advantage of auto-complete.\n\n\n# list packages in a vector and load them all\npkgs <- c(\"readr\", \"cowsay\")\npurrr::walk(pkgs, require, character.only = TRUE)\n\n# read quotes from a url\nf_read_data <- function(url){\n  suppressMessages(\n    quotes  <- read_csv(url)  \n  )\n  return(quotes)\n}\n\n# paste the quote to the author\nf_preprocess_data <- function(d){\n  d$full_quote  <- paste0(d$Quote, \" -\", d$Author)\n  return(d)\n}\n\n# print a random animal and a random quote\nf_inspire_me <- function(d){\n  animals <- names(animals)\n  say(sample(d$full_quote, 1), by = sample(animals, 1))\n}\n\n\n\nWe can call this script using source() to load or import these functions into our environment where they are available for use, just as we load a library.\n\n\nsource(here(\"functions.R\"))\n\n\n\nAbstracting Functions from Code\nHowever, this is hardly a satisfying solution because in a real project, our pretend functions above may grow quite large, and we will likely add more and more functions. Eventually, a single script may hold them all, and something like functions.R may become many hundreds of lines long, making it difficult to sift through, debug, or add new lines of code. A better organizational approach which makes things easier to maintain over time is to move all our functions to a directory /functions, and store them all as separate files named after their function name:\n\nA good rule of thumb to follow is to try and keep scripts less than 150 lines in length. When scripts approach this length, they become difficult to keep track of, and there were likely missed opportunities to refactor the script into separate functions and modules.\nSave as /functions/f_read_data.R\n\n\n# read quotes from a url\nf_read_data <- function(url){\n  suppressMessages(\n    quotes  <- read_csv(url)\n  )\n  return(quotes)\n}\n\n\n\nSave as /functions/f_preprocess_data.R\n\n\n# paste the quote to the author\nf_preprocess_data <- function(d){\n  d$full_quote  <- paste0(d$Quote, \" -\", d$Author)\n  return(d)\n}\n\n\n\nSave as /functions/f_inspire_me.R\n\n\n# print a random animal and a random quote\nf_inspire_me <- function(d){\n  animals <- names(animals)\n  say(sample(d$full_quote, 1), by = sample(animals, 1))\n}\n\n\n\nThe functions folder in the root project directory should now look like this:\n\n\n\nNow in our /code directory, we create a script, 01_control.R to source our functions and use them. Be sure to restart R to clear your environment before sourcing this control script so we know we are working from a clean slate.\nSave as /code/01_control.R and run.\n\n\n# packages needed for this script\npkgs <- c(\"readr\", \"cowsay\", \"here\", \"tidyverse\", \"glue\")\npurrr::walk(pkgs, require, character.only = TRUE)\n\n# silently source all functions using the purrr::walk function\nwalk(list.files(here(\"functions\"), full.names = TRUE), ~source(.x))\n\n# define the url where quotes are located\n# get pieces to make link\nrepo <- \"JakubPetriska/060958fd744ca34f099e947cd080b540\"\ncsv <- \"raw/963b5a9355f04741239407320ac973a6096cd7b6/quotes.csv\"\nurl <- glue(\"https://gist.githubusercontent.com/{repo}/{csv}\")  \n\n# use all of our functions\nf_read_data(url) %>% \n  f_preprocess_data() %>% \n  f_inspire_me()\n\n\n\n ----- \nWho we are never changes. Who we think we are does. -Mary Almanac \n ------ \n    \\   \n     \\\n     .-.\n    (o o)\n    | O \\\n     \\   \\\n      `~~~' [nosig]\n  \n\nsource() is the key to chaining together many scripts. In the example above, we were able to abstract functions into a separate folder which makes keeping track of them much easier than if they cluttered our control script.\n\nIt’s also a good rule of thumb to keep each line of code less than 75 characters long to ease readability.\n\nLearn more\n\nSeparating all functions into standalone scripts is not a revolutionary idea – in fact, this is precisely how R packages are written! For example, see the {dplyr} github repo’s /R folder which contains all dplyr functions in one directory. When you call library(dplyr) you’re essentially sourcing all of these functions into your environment.\n\n\nIf project management and reproducible data pipelines are interesting to you, check out the {targets} R package. A similar framework for Shiny Apps exists called {golem}, which also includes {usethis}-like commands that streamline common chores in Shiny App development.\n\n\n{renv}\nWe use {here} because we expect that whoever else opens your code on their machine is likely to have a different project root path, and {here} ensures your code is portable between different computers with different root project paths (e.g., ~/Documents/Github/myproject versus C:\\Users\\louis\\Documents\\myproject).\nDevelopment environments are similar. When we work in R – or any programming language for that matter – we use a snapshot of package versions based on when we downloaded and installed them [e.g. with install.packages()]. You can check the version of the installed packages loaded into your current environment with sessionInfo().\n\n\nsessionInfo()\n\n\nR version 4.0.2 (2020-06-22)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Catalina 10.15.6\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] forcats_0.5.1   stringr_1.4.0   dplyr_1.0.4     purrr_0.3.4    \n [5] tidyr_1.1.2     tibble_3.1.0    tidyverse_1.3.0 here_0.1       \n [9] readr_1.4.0     glue_1.4.2      cowsay_0.8.0    knitr_1.30     \n[13] ggplot2_3.3.3  \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.1.0  xfun_0.22         haven_2.3.1      \n [4] colorspace_2.0-0  vctrs_0.3.7       generics_0.1.0   \n [7] htmltools_0.5.1.1 usethis_2.0.0     yaml_2.2.1       \n[10] utf8_1.2.1        rlang_0.4.10      gridtext_0.1.4   \n[13] pillar_1.5.1      withr_2.4.1       DBI_1.1.1        \n[16] dbplyr_2.1.0      readxl_1.3.1      modelr_0.1.8     \n[19] fortunes_1.5-4    lifecycle_1.0.0   cellranger_1.1.0 \n[22] munsell_0.5.0     gtable_0.3.0      rvest_0.3.6      \n[25] evaluate_0.14     curl_4.3          fansi_0.4.2      \n[28] broom_0.7.0       Rcpp_1.0.6        scales_1.1.1     \n[31] backports_1.1.10  jsonlite_1.7.2    fs_1.5.0         \n[34] rmsfact_0.0.3     distill_1.2       hms_1.0.0        \n[37] png_0.1-7         digest_0.6.27     stringi_1.5.3    \n[40] grid_4.0.2        rprojroot_1.3-2   cli_2.4.0        \n[43] tools_4.0.2       magrittr_2.0.1    crayon_1.4.1     \n[46] pkgconfig_2.0.3   downlit_0.2.1     ellipsis_0.3.1   \n[49] xml2_1.3.2        reprex_0.3.0      lubridate_1.7.9.2\n[52] httr_1.4.2        assertthat_0.2.1  rmarkdown_2.7    \n[55] rstudioapi_0.11   R6_2.5.0          ggtext_0.1.1     \n[58] compiler_4.0.2   \n\nThe version number is the string of numbers listed after a package name and underscore.\nSimilarly, you can use installed.packages() to view information on all of your installed packages.\nWhen packages change between versions, changes are typically designed to fix bugs or improve performance, but sometimes, they can break code. Thus, collaborative work on a project may be challenged by people working on the same code but with different versions of packages.\n\nEven the version of R itself changes, although base R changes very slowly and the R Core Team tries to make new versions of R backwards-compatible to not break scripts written before potentially breaking changes.\nThe solution to this problem is for everyone to use the same versions of packages (and R), which is to say that collaborators should use the same development environment. This is a common concept across programming languages.\n{renv} manages your package environment and makes it easy to share it with others by creating and curating a “lock” file (renv.lock) in the root project directory. When starting a project, create the file with renv::init(), install packages as you go along, and update the lockfile with renv::snapshot(). When a collaborator opens your project (for example, after cloning it from Github), all they need to do is open the .RProj file and {renv} will automatically set up the development environment captured in the lock file.\n\nThe renv.lock file is a JSON file with information on the version of R and the versions of all packages used by the project.\nIf you find yourself needing to share important analyses, perhaps that run on a production server, you should look into {renv}. For most day-to-day data science that you don’t plan on sharing or working collaboratively on, it may be unnecessary.\n\n\nPrevious module: Introduction Next module: Git\n\n\n\n",
      "last_modified": "2021-05-18T17:51:00-07:00"
    },
    {
      "path": "m_reproducible_workflows.html",
      "title": "From iteration to functional programming",
      "description": "Conceptual code \n",
      "author": [],
      "contents": "\n\nLearning objectives\nA\nB\nC\n\n\n\nPrevious module:Simple Shiny Next module:Approaches to work with Complex Data\n\n\n\n",
      "last_modified": "2021-05-13T18:34:16-07:00"
    },
    {
      "path": "m_simple_shiny.html",
      "title": "Simple Shiny",
      "description": "How to create interactive dashboards with R\n",
      "author": [],
      "contents": "\n\nContents\nWhat is a Shiny App and Why is it useful?\nCase Study: Sacramento county groundwater elevation\n\nBasic Shiny App Structure\nCreating a Shiny App\nui (frontend)\nControl widgets\nOutputs\n\nserver (backend)\nReactivity\nRender objects\n\nPutting it all together\nShare and deploy a Shiny App\nExtending Shiny\nAdditional Resources\n\n\n\nLearning objectives\nUnderstand what a Shiny App is and why you might want or need to build one\nUnderstand the basic structure of a Shiny App and common hiccups\nDiscuss approaches to extend your Shiny skills\n\n\nWhat is a Shiny App and Why is it useful?\nAccording to the Mastering Shiny book:\n\nShiny is a framework for creating web applications using R code. It is designed primarily with data scientists in mind, and to that end, you can create pretty complicated Shiny apps with no knowledge of HTML, CSS, or JavaScript.\n\nShiny Apps are made using the {shiny} package.\nBecause they extend R-based analyses, interactive Shiny Apps have as many diverse niches and uses as the the R community itself, and it’s likely that you have come across a Shiny App in the wild before.\nShiny Apps are highly customizable, and allow R users to wrap existing code and data with an additional layer of interactivity using R code from the {shiny} package to better visualize, analyze, export, and more. This allows any user (even non-R users!) to interact with the data, providing a powerful way of exploring and understanding the data within the Shiny App.\nNow that we know what a Shiny App is, a fundamental question that we need to answer is “when and why should I build a Shiny App?” Generally, you should create a Shiny App when you want to make results and/or data available to others, or when the dataset is complex enough to warrant a Shiny App to ease exploration. To provide a specific example, let’s examine a case study.\nCase Study: Sacramento county groundwater elevation\nImagine you’re analyzing groundwater levels across California1 and want to assess groundwater elevation trends over time in Sacramento County. You may begin with an EDA to filter the data to Sacramento County, clean the data, and make some exploratory plots.\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(sf)\n\n# read groundwater level data pre-filtered to Sacramento county\ngwl <- read_csv(here(\"data\", \"gwl\", \"gwl_sac.csv\")) %>% \n  st_as_sf(coords = c(\"LONGITUDE\",\"LATITUDE\"), crs = 4269)\n\n# read sacramento county shapefile and reproject\nsac <- st_read(here(\"data\", \"shp\", \"sac\", \"sac_county.shp\"), quiet=TRUE) %>% \n  st_transform(4269)\n\n# plot the groundwater levels at each monitoring site \ngwl %>% \n  ggplot() +\n  geom_line(aes(MSMT_DATE, WSE, group = SITE_CODE), alpha = 0.5)\n\n\n\n# slice the first station per group of groundwater level observations\n# (see comment aside)\ngwl_min <- gwl %>% \n  group_by(SITE_CODE) %>% \n  slice(1) %>% \n  ungroup() \n\n# visualize sites on a map\nggplot() +\n  geom_sf(data = sac) +\n  geom_sf(data = gwl_min, alpha = 0.5, color = \"blue\") +\n  theme_void()\n\n\n\n\nNotice that nrow(gwl) is greater than nrow(gwl_min), but length(unique(gwl$SITE_CODE)) equals length(unique(gwl_min$SITE_CODE)). We don’t need to plot all of the redundant gwl locations on the map, so we created a minimal tibble with only the unique SITE_CODEs, and hence, geometry.\nDuring the analysis, you realize you want to easily look at data by monitoring site, so you make a function that streamlines this.\n\n\n# function that takes a site code and generates a plot\nf_make_hydrograph <- function(data, site_code){\n  p <- data %>% \n    filter(SITE_CODE == site_code) %>% \n    ggplot(aes(MSMT_DATE, WSE)) +\n    geom_line(alpha = 0.5) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    labs(title = glue::glue(\"Groundwater level (ft AMSL) at: {site_code}\"))\n  return(p)\n}\n\n# make a hydrograph for the first SITE_CODE\nf_make_hydrograph(gwl, gwl$SITE_CODE[1])\n\n\n\n\nYour supervisor requests figures from certain stations, and you find yourself re-running this function to generate them. You also have teammates that require subsets of these data per monitoring site, with specific columns in the output. You add a function that writes data to csv files to handle this task, but realize that this project is ongoing, and people will keep coming to you with requests for alternate combinations of plots or data. Also, the groundwater level data is sufficiently large and complex that you want to be able to view them all in one place.\n\nOne solution to automate the “data visualization exploration” and “data sharing” processes is to create a Shiny App.\n\nIn the sections that follow, we will create a Shiny App that allows users to easily select and visualize groundwater data at different monitoring stations, and export data for a selected site.\n\nBasic Shiny App Structure\nAt the bare minimum, a Shiny App is an .R file or set of .R files with three components: a ui, server and runApp(). The runApp() function takes the ui and the server objects and runs the Shiny web application.\nBy design, Shiny Apps separate front and back end components of the web application. The ui stands for “user interface” and defines front-facing components that the user sees and interacts with, like plots, tables, sliders, buttons, and so on. The server holds the back-end logic that accepts input from the user, and uses these inputs to define what data transformations should occur and what is passed back to the frontend for the user to view and interact with.\nIn order to demonstrate a simple Shiny App, we will use a single file called app.R defines a ui and server. Afterward, we will also discuss approaches to modularize an app into separate files - this may be necessary if an app becomes sufficiently complex.\nCreating a Shiny App\nThe most simple way to create an app within RStudio is to click File > New File > Shiny Web App….\n\n\n\nBy default, RStudio will create a new folder with a name you provide in your project directory2. Enter gwl_shiny as the “Application name” and click Create.\n\n\n\nYou should now have a folder called gwl_shiny in the project directory, which you can verify in the File Viewer Pane. Inside that folder should be a single file, app.R.\n\n\n\nA default Shiny App is contained in this file. We can run all of the code to view the app on our local machine, or click the Run App icon in the top right corner of the code editor.\n\n\n\nScroll through app.R and notice that there is a ui object, a server object which is a function of input and output, and a runApp() function which takes the ui and server objects as input.\n\nui (frontend)\nThe ui defines what the user sees when they interact with the Shiny App. Generally speaking, the ui shows:\ncontrol widgets (the interface) which allow the user to send commands to the server\noutput from the server\nControl widgets\nConsider the following example App, which demonstrates a few of the control widget3 inputs available for Shiny Apps.\n\n\nlibrary(shiny)\n\nui <- fluidPage(\n    # sidebar with generic example inputs\n    sidebarLayout(\n        sidebarPanel(\n            sliderInput(\n              \"i_slider\",     # control widget ID     \n              \"Slider Input\", # label above the widget\n              min = 1,\n              max = 50,\n              value = 30\n            ),\n            fileInput(\n              \"i_file\",       # control widget ID     \n              \"File Input\"    # label above the widget\n            ),\n            dateInput(\n              \"i_date\",       # control widget ID     \n              \"Date Input\"    # label above the widget\n            ),\n            textInput(\n              \"i_text\",       # control widget ID     \n              \"Text Input\"    # label above the widget\n            ),\n            selectInput(\n              \"i_select\",     # control widget ID     \n              \"Select Input\", # label above the widget\n              choices = c(\"A\",\"B\",\"C\")\n            ),\n            radioButtons( \n              \"i_radio\",      # control widget ID  \n              \"Radio Buttons\",# label above the widget\n              choices = c(\"A\",\"B\")\n            )\n        ),\n        mainPanel(\"Look at all those inputs!\")\n    )\n)\n\n# blank server logic - the App does nothing with \n# control widget inputs and creates no output!\nserver <- function(input, output) {}\n\n# run the Shiny app\nshinyApp(ui = ui, server = server)\n\n\n\n\n\n\nThese control widgets may be intuitive to you, as we’ve all interacted with web-based tools and apps that use widgets like these. View a more comprehensive list of control widgets here.\nChances are that you won’t use all of the control widget inputs available in any given Shiny App you build. The choice of inputs you use will depend on what information you want to present to the user, and how information they select with those widgets should modify data in the server.\nOutputs\nA ui with only control widgets (i.e., the App example above) is like having a keyboard without a monitor. You can change inputs by typing on the keyboard, but can’t see anything happen without a monitor! In this analogy, the monitor is a set of outputs. Your Shiny App users will spend a lot of time interacting with outputs, and most of our time spent designing and creating a Shiny App will go into creating outputs. To view default Shiny outputs, type shiny::output and see what comes up in tab complete. For example, plotOutput() is used to render plots, textOutput() is used to render text, and so on.\n\n\n\nThere are other types of output not included in the {shiny} package for objects created by other packages, for example leaflet::leafletOutput() and DT::dataTableOutput() create leaflet and DT objects.\nEach output in the ui takes as a first argument the “output id” as a character string, for example, plotOutput(\"my_plot\") means a plotOutput() with an “output id” of \"my_plot\".\n\n\nFix the App\n\nEvery output in the ui should have a corresponding object rendered in the server. Thus, if we define a Shiny App with plotOutput(\"my_plot\") in the ui, we need to assign a rendered plot to the object output$my_plot in the server. For practice, fix the following Shiny App. You’ll know you were successful if you see a plot when running the App.\n\n\n\nlibrary(shiny)\n\n# user interface\nui <- fluidPage( plotOutput(\"plot\") )\n\n# server logic that draws a plot\nserver <- function(input, output) {\n  output$my_plot <- renderPlot({ plot(1:10) })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\n\nClick for the Answer!\n\nThis very boring Shiny App illustrates a very critical concept. It has no control widget inputs, but it does have a matching output id and output object in the ui and server respectively.\nThe answer, therefore, is frustratingly simple: plotOutput(\"plot\") should be plotOutput(\"my_plot\").\n\n\nlibrary(shiny)\n\n# user interface\nui <- fluidPage( plotOutput(\"my_plot\") )\n\n# server logic that draws a plot\nserver <- function(input, output) {\n  output$my_plot <- renderPlot({ plot(1:10) })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\n\n\n\nserver (backend)\nIf the ui control widgets are like the keyboard receiving inputs, then the server is the computer brain of the Shiny App that takes inputs, translates the information from the inputs (code that does something with the data input), and creates outputs, and finally sends these back to the ui where a user can interact or see the result.\nReactivity\nReactivity4 is a driving concept in Shiny App programming. The fundamental idea is that what happens in a Shiny App depends only on values that change, and parts of the App are interconnected such that when inputs change, these inputs trigger a re-evaluation of code that regenerates outputs, until the inputs change again, and so on.\nRender objects\nIn the ui we saw that each output ID created by the server had a unique ID name and was wrapped in an “output” function. For example, plotOutput(\"my_plot\") had an object in the server called output$my_plot that was assigned the result of a renderPlot({ }) function, and inside that function was the code for our plot. Each “render” function has a corresponding “output” function (e.g., renderText() and textOutput() go together). To view some of the default “render” functions, type shiny::render and view the options available in the tab complete popup.\n\n\n\n\nPutting it all together\nLet’s take what we’ve covered and put it all together to address the Case Study introduced above. We will build a simple Shiny App with a plotOutput() and DT::dataTableOutput() that shows the hydrograph and data for a user-specified monitoring point. We’ll also include data download buttons within the data table and demonstrate how to make the plot interactive with {plotly}.\n\n\nlibrary(shiny)\nlibrary(shinythemes)\nlibrary(tidyverse)\n\n# load sac county groundwater data and sac county polygon\ngwl <- read_csv(here::here(\"data\", \"gwl\", \"gwl_sac_shiny.csv\"))\n\n# ------------------------------------------------------------------------\n# user interface\nui <- fluidPage(\n\n    # change default theme to \"united\"\n    theme = shinytheme(\"united\"),\n  \n    # application title\n    titlePanel(\"Sacramento County Groundwater Level Data\"),\n\n    # sidebar with a dropdown input for site_code\n    sidebarLayout(\n        sidebarPanel(\n            selectInput(\"site_code\",\n                        \"Select a site code:\",\n                        choices = unique(gwl$SITE_CODE))\n        ),\n\n        # tabs with hydrograph and data table\n        mainPanel(\n            tabsetPanel(\n                tabPanel(\"Hydrograph\", plotly::plotlyOutput(\"gwl_plot\")),\n                tabPanel(\"Data\", DT::dataTableOutput(\"gwl_data\"))\n            )\n        )\n    )\n)\n\n# ------------------------------------------------------------------------\n# define server logic to plot a hydrograph and create data table\nserver <- function(input, output) {\n\n    # --------------------------------------------------\n    # create hydrograph\n    output$gwl_plot <- plotly::renderPlotly({\n\n        # draw the ggplot based on the \"site_code\" user input\n        p <- filter(gwl, SITE_CODE == input$site_code) %>%\n            ggplot(aes(MSMT_DATE, WSE)) +\n            geom_line(alpha = 0.5) +\n            geom_smooth(method = \"lm\", se = FALSE) +\n            labs(title = input$site_code,\n                 x = \"\", y = \"Groundwater level (ft AMSL)\")\n\n        # render the plotly object\n        plotly::ggplotly(p)\n    })\n\n    # --------------------------------------------------\n    # create data table\n    output$gwl_data <- DT::renderDataTable({\n\n        # draw the plot based on the \"site_code\" user input\n        DT::datatable(\n            filter(gwl, SITE_CODE == input$site_code),\n            extensions = 'Buttons',\n            options =\n                list(dom = 'Bfrtip',\n                     buttons = c('copy', 'csv', 'excel', 'pdf', 'print'))\n        )\n    })\n}\n\n# ------------------------------------------------------------------------\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\n\nThe sidebarLayout() and tabsetPanel() functions demonstrated in this module are just a few of the many layout options to configure the placement of control widgets and outputs in your Shiny Apps.\nShiny App design and customization options are vast. We demonstrated the “united” theme here, which is one of many {shinythemes}. You can also add custom CSS in-line, or in a styles.css script in the Shiny App directory.\nFinally, although we demonstrate a simple Shiny App, more functional and complex Apps may benefit from modularizing particular App components. Learn more about building Shiny modules here.\nLet’s break down what we just did.\nBefore anything else, we loaded the packages we needed5 and our groundwater level data gwl.\nNext, in the ui, we added a custom theme from the {shinythemes} package to change the default appearance of our App. We used a selectInput() control widget with a ID \"site_code\" and choices equal to the unique values of SITE_CODEs in the gwl dataframe. Then, in the mainPanel() we added a tabsetPanel() with two tabPanel() tabs: one with a plotly::plotlyOutput() plot, and another with a DT::dataTableOutput() output.\nFinally, in the server, we rendered plotly and dataTable objects and saved them to the output object under the output IDs \"gwl_plot\" and \"gwl_data\". These objects are displayed in the ui.\n\nShare and deploy a Shiny App\nUp until now, we’ve been using our own computer to run our Shiny App example. How do we share a Shiny App with others?\nThe fastest and simplest way to share your Shiny app with another R user is to share the app.R file and data dependencies. They can open the file and run the app on their computer as we’ve been doing in this module. However, we may want to share our app with a wider, non-technical audience over the internet. When we move a Shiny App from our computer onto a web server so that it can be shared over the internet, we deploy the app.\nThere are a range of services that allow us to deploy Shiny Apps that range from simple to complex, and costly to inexpensive.\nA relatively painless way to deploy is via shinyapps.io, run by RStudio. A free tier is available, and provides a limited number of “live hours” per month for users to interact with your App. Paid tiers afford more “live hours” per month, a faster web server, and features like password protection.\n{shiny} is open source, and in that spirit, the open source Shiny Server can be installed on any local or cloud server (e.g., AWS, Google Cloud, Microsoft Azure). If you don’t have a background in cloud computing, your System Administrator (Sys Admin) can help you move your App onto a cloud or company server6. Although this option requires more manual setup than shinyapps.io, it offers a non-commercial alternative, and perhaps more control depending on your needs.\n\nExtending Shiny\nCongratulations! You’re now familiar with the basics of {shiny}. This module is just the tip of the iceberg. There are so many ways to extend {shiny}, and the opportunities for customization of Shiny Apps are vast. We recommend the following ways to increase your knowledge in this domain:\nfind open source Shiny Apps that you admire, clone their source code to your machine, run the App, and change things so you can explore how the App works. Borrow bits of code in new Apps you create\nfind a side project that’s a good candidate for a Shiny App and build one - it doesn’t need to be work-related; in fact, it might be more fun to work on something tangential that you find interesting\nexplore the free online books and guided video tutorials in the links below\nAdditional Resources\nBelow are a few freely available books, presentations, and locations to find Apps online:\nMastering Shiny: Free online book that serves as an authoritative guide on how to build, maintain, deploy, and customize R Shiny Apps. Material goes well beyond what is presented in this module.\nGuided Shiny Video tutorials: Series of guided video tutorials with live-code demos and slides that explain the fundamentals of R Shiny Apps and how to use them.\nRStudio Shiny Gallery: Examples of Shiny Apps highlighting different ways Shiny has been used, with links to source code\n\n\nPrevious module:Interactive Visualization Next module:Reproducibility & Automation\n\nFor instance, imagine you are using the California Department of Water Resources’ Periodic Groundwater Level Database used throughout this course.↩︎\nIf using {here}, find your project directory by running here().↩︎\nA control widget is a tool that allows users to send information from the frontend ui to the backend server. Different widgets allow different types of information to pass between the ui and server. Notice that the first argument to each control widget is the unique ID or name of that widget. IDs must be unique, and are accessed in the server in a named list called input. For example, the sliderInput(\"bins\") has the unique ID \"i_slider\" and the user-input value to this widget can be accessed from within the server with input$i_slider.↩︎\nA comprehensive treatment of reactive programming in Shiny is beyond the scope of this module, but we recommend reading this article, and this book section to learn more about reactive graphs when you decide to start building more complex Shiny Apps.↩︎\nAlthough we don’t cover it in this module, you can also split your App into 3 files: ui.R (for the ui), server.R (for the server), and global.R (loads all App dependencies). When an App becomes big and complex, splitting it up into smaller parts can make it easier to develop.↩︎\nThe Shiny Server Administrator’s Guide](https://docs.rstudio.com/shiny-server/)) provides detailed documentation geared towards System Administrators that will help in deploying a Shiny App.↩︎\n",
      "last_modified": "2021-05-13T18:34:16-07:00"
    },
    {
      "path": "m_updating_r.html",
      "title": "Updating R and R packages",
      "description": "How to check versions and update your installation\n",
      "author": [],
      "contents": "\n\nContents\nChecking Versions and Updating\nCheck your    Version\nCheck/Update  \n\nUpdating R Packages\nBest Practices for Updating\n\n\nLearning objectives\nUnderstand R versions and how to check what you have\nLearn how to update your R/RStudio installation\nLearn how to update R packages\n\nChecking Versions and Updating\nAs packages and R continue to improve, new versions of R are released1. In R, major versions are released infrequently (i.e., 3.0.0 was released in April 2013 and 4.0.0 was released in April 2020), but minor versions are released more regularly (4.0.4 was released in February 2021 and 4.0.5 was released in March 2021).\nAt minimum it’s advisable to maintain and update to the most recent major version of R, as these often contain important security/programming changes. Depending on your workflow and package needs, it’s good practice to keep the most recent minor version as well, though it’s not uncommon to maintain multiple minor versions of R if your analyses or workflows depend on specific versions of a package.\nCheck your    Version\nAn easy and quick way to check what version of R you have and the most recent available version is first to open R, type R.version, hit enter, and see what you get:\n\n\nR.version\n\n\n               _                           \nplatform       x86_64-apple-darwin17.0     \narch           x86_64                      \nos             darwin17.0                  \nsystem         x86_64, darwin17.0          \nstatus                                     \nmajor          4                           \nminor          0.2                         \nyear           2020                        \nmonth          06                          \nday            22                          \nsvn rev        78730                       \nlanguage       R                           \nversion.string R version 4.0.2 (2020-06-22)\nnickname       Taking Off Again            \n\nThis command will return information about the R version your system is using, as well as some information specific to your operating system (os).\nNext, visit the R CRAN website to see what the most recent version is. If you haven’t updated recently, go ahead and grab the most recent R version for your system and install it.\nWhen you update a major version of R, your personal library of packages will no longer work, and you will need to reinstall all of your packages. There are a few tools you can use to do this like . This work to stay up-to-date happens fairly infrequently.\nCheck/Update  \nWe can check our version of RStudio by going to the toolbar at the top of your RStudio window and clicking Help > Check for Updates. If you have the most recent version, there will be a message box letting you know as much. If not, RStudio will direct you to their webpage to download the most recent version.\nWhile it isn’t always necessary to update to the most recent version of R, there’s no reason not to always use the most recent stable version of RStudio. It will generally include the best features and up-to-date fixes.\nUpdating R Packages\nAs we use R more regularly for different tasks, it’s common to accumulate many R packages in our . Every package is maintained and updated on a different schedule than R and RStudio, and so as new functions and features are written, or bugs are fixed, packages will be updated intermittently. Some package maintainers do this regularly, others may do it sporadically. Either way, we will typically need to update packages semi-regularly.\nFiles associated with installed R packages are located at .libPaths(). View installed packages with list.files(.libPaths()).\nThere are several methods of updating your R packages. If updating via RStudio, go to the toolbar at the top and select Tools > Check for Package Updates.... Depending on how many packages you’ve installed, and how recently you updated things, a window will appear saying either All packages up-to-date! or something that looks more like this:\nAs a rule of thumb, whenever updating your R version, it’s best to update/install your R packages too!\n\n\n\nFigure 1: Package update window\n\n\n\nWe can choose to Select All and update everything at once, or selectively update things.\nAfter we click Install Updates we may typically also see a message like this:\n\n\n\nFigure 2: Restart R\n\n\n\nYou can choose to cancel, but it’s fine to click Yes and proceed. Sometimes we will also get something in the Console that will ask the following:\n\nDo you want to install from sources the packages which need compilation? (Yes/no/cancel)\n\nSome packages require this, so generally it’s okay to type Yes and hit enter to proceed. At this point we can wait and let R/RStudio update things for us. Depending on how many packages are involved, this can take a few minutes. Importantly, when it’s all said and done, make sure there weren’t errors or issues with packages that didn’t get installed. We can use Tools > Check for Package Updates... again and see what packages remain, or if we get a message saying all packages are up-to-date.\nBest Practices for Updating\nWe recommend the following approach to updating R packages (check out the great rstats.wtf chapter on this topic and more):\nUpdate packages frequently (weekly)\nUpdate R quarterly (or at least check)\nFor complete reproducibility for a project check out the {renv} package\nDon’t copy packages over between R versions–start clean and fresh\nUpdating packages can be irksome, but it’s typically for the best. We advise updating your packages frequently if you regularly use R (e.g;, a weekly checks and updates to packages). If you want to be sure you have the exact package version for future use and reproducibility, the {renv} may be a great solution for an analysis or report, but may not be necessary if your aren’t using or working on a specific project/analysis. {renv} keeps a record of your R packages within an RProject down to the exact version number when you originally loaded these packages, and allows you to update this record as your project evolves over time.\nRead more about {renv} in the project management module.\nOverall, treat R and R packages as something that will be highly functional with minimal but regular maintenance, like a vehicle that needs new tires or an oil change once in a while. This will keep things running smoothly over the long-term.\n\n\n\nFigure 3: Success over time! (Artwork by @allison_horst)\n\n\n\n\nSee a list of previous versions here.↩︎\n",
      "last_modified": "2021-05-16T13:12:05-07:00"
    },
    {
      "path": "m_version_control.html",
      "title": "Version Control with `git`",
      "description": "A time machine you can share...\n",
      "author": [],
      "contents": "\n\nContents\nThe case for version control\nUsing version control\n\nLearning git through a project\nSetup git locally\nCreate a Repository\nCollaborative Github Settings\nBranches & Conflicts\nCreate a branch\nDownload Data: add, commit, push\nPull Requests\nfetch and pull\n\nClosing the loop\nAdditional Git Details\nResources & Tutorials\n\n\nLearning objectives\nUnderstand and use basic git commands for version control\nDevelop a mindset and approach to set up and manage a version controlled project\nIdentify best practices for using git collaboratively\n\nThis lesson assumes you have:\nSet up a Github Account\nInstalled git on your computer locally\nThe case for version control\nAs water data scientists, we will work independently and with others on documents, data, and code that change over time. Version control is a framework and process to keep track of changes made to a document, data, or code with the added bonus that you can revert to any previous change or point in time and you can work in parallel with multiple people on the same material. In this module, we will practice version control with git, which is perhaps the most common version control system available today.\nThe important difference between using a tool like git and Github versus cloud-based services (e.g., Dropbox, Google Docs) which are in a way, version control systems, is the ability to control exactly what changes and/or files get “versioned”. These versions are snapshots of your work with unique identifiers along with a short commit message, which allows us to restore these changes at any point in time. Thus, git offers much finer control over specific changes by specific users; this makes version control a very powerful tool (as well as a computer lifesaver when things go awry!).\nWhat is the difference between git and Github?\ngit is a version control program that is installed on your computer with associated commands we use to interact with version controlled files.\nGithub is a website that interfaces with git and allows us to store/access/share our files as repositories1.\nThere is a rich and complex set of tools and functionality with git and Github, but in this module, we’ll focus on the basics and link to more material.\nUsing version control\nSome folks may just use version control as another way to backup their work2, but version control really shines when working on a collaborative project. For example, many are familiar with the situation below:\nFigure from http://www.phdcomics.com/comics/archive/phd101212s.gif\nThe comic above shows a single thesis document, but really the situation isn’t any different than a big data analysis project. A common series of steps that are generally taken by multiple people, may include:\nDownload/gather data\nClean/transform data\nAnalyze and visualize data\nProduce a front facing product or report\nMany of these tasks (or subtasks) may overlap or depends on one another. Complex collaborative projects require forethought in how to set things up so everyone can seamlessly stitch their contribution into the overall fabric of a project without holding up the process for other team members.\nWith version control, if changes you make break things, it’s easy to revert or get back to an earlier working version using the commit timeline, because we have access to the history of our changes.\n\nPause and Discuss!\n\nIf you had 10 people to work on a big project with steps outlined above, how would you approach it? How would you break out these pieces or subtasks? How would you track progress?\n\n\n\nThis module will cover version control with git, including the basic commands, but it’s important to remember:\n\nUnderstanding and learning how to approach, setup, and implement collaborative projects with version control are just as important as learning git commands\n\nThus, successful version control is not just about effectively using tools like git, but also developing skills to organize projects, collaborate with others, and implement version control so it’s useful (not just another way to back things up…see below).\nLearn to leverage the power of git for things other than just a backup! (Figure from https://xkcd.com/1597/)\n\n\n\nLearning git through a project\nSometimes the best way to learn is to walk through a real-world example. Let’s walk through a simplified example of a common set of tasks a team may face using R and learning git along the way.\nLet’s imagine we have a team of 3 folks, Jo, Mo, and Main. Each has a unique skill set, and each knows how to use R and git. The team is tasked with downloading and visualizing flow data for a specific river gage to provide status report updates on a monthly basis, which are used for various regulatory actions (i.e., how much water is available for diversion, how flows relate to period of record averages, etc).\n\n\n\nFigure 1: An example team and workflow.\n\n\n\nLet’s get this team started on their! First we need to make sure everything is setup and installed.\nSetup git locally\nWe highly recommend Jenny Bryan & Jim Hester’s happygitwithr website because it covers everything (and more) that you may encounter when using git with R. In particular, please take a moment to check/update your git installation and configure Github. The following links will help with this:\nInstall Git\nIntroduce yourself to Git\nThe {usethis} package is an excellent option to help you get things setup within R, like this:\n\n\nlibrary(usethis)\n\n# use your actual git user name and email!\nuse_git_config(user.name = \"yoda\", user.email = \"yoda2021@example.org\")\n\n\n\nRemember, we only need to do this once!3\nCreate a Repository\nThere are several ways to create repositories on Github and with RStudio4. We recommend the Github first option as it’s easiest to intialize and manage.\nTo begin, go to Github.com and login. From there, let’s create a new repository!\n\nChallenge\n\nPlease take a moment and create a new repository on Github following the happygitwithr.com instructions.\nName the repository water-is-life\nCheck the Add a README file box at the bottom\nMake this a public repository\n\nClick for Answers!\n\nHopefully you see something like this before you click the green Create Repository button.\n\n\n\nAnd then afterwards, a screen like this:\n\n\n\n\nThe last piece to this is getting this repository which is currently in the “cloud” to a local copy on your computer. This is called cloning. To clone the repository to your computer, we need to do the following:\nCopy the repository address. Make sure you’ve clicked HTTPS and the link in the box starts with https://. Copy that link to your clipboard.\n\n\n\nNow open RStudio and navigate to File > New Project > Version Control > Git\nIn the Repository URL box, paste the link in, and hit Tab. The Project directory Name should automagically fill with the name of the repository. Go ahead and put this where you want it to live locally5 and hit Create Project. Open in new session is optional, but allows you to keep existing projects open.\n\n\n\nGreat! We should now have an RStudio project that is version controlled with git, and there is a local copy we can work with on our computer.\nCollaborative Github Settings\nSince we are working on a collaborative project, we need to make sure our collaborators have access to make changes to our repository. So, if Main created the repository, they would need to add fellow collaborators to the repository. Let’s do that now:\nGo to Github.com and login\nNavigate to your repository water-is-life\nClick on the Settings tab in the upper right corner\nYou can view and clone (copy) any public Github repository. However, you can only commit changes if you are the owner, or have been added as a collaborator (more on this later).\n\n\n\nFigure 2: Click on the Settings tab in the upper right corner to add collaborators.\n\n\n\nClick on Manage Access on the side bar on the left (you may need to enter your password or authentication again)\n\n\n\nFigure 3: Then click on Manage Access.\n\n\n\nClick on Invite Collaborator and enter the username or email you want to add!\n\n\n\nFigure 4: Invite collaborators by email or username!\n\n\n\nBranches & Conflicts\nOften when teaching git, we start with the basic commands. However, it’s important to consider one of the more powerful parts of using git is the ability to use branches. Branches, or branching, in git, is a way for multiple folks to work on the same repository independently, and a standardized way to integrate those changes back into the repository. Every repository typically starts with one single main branch. This is like the trunk of the tree, or the main train track. We can create additional branches off of this track, and if we want to merge them back in to make them available for our collaborators, we use something called a pull request. This is essentially a way to double check if there will be any conflicts between the work in the branch and the work in the main branch6, and it also provides a way to document and review any changes. If a pull request does not create merge conflicts (don’t worry about this now, we will cover it later) and if a collaborator on the repository approves the pull request, then the branch is “pulled” (or merged) into the main branch, and then typically deleted (since the branch has served its purpose, and the changes in it are now reflected in the main branch).\nEven if you are the only person working in a repo, it’s a good habit to use branches because if you realize the branch/work you are doing is a dead end, you can always delete a branch and return to the main branch to pick things up where you last left off.\n\n\n\nFigure 5: Example of multiple branches in a project splitting off the main branch\n\n\n\nCreate a branch\nIn our example we want to make sure we keep our main branch clean and up-to-date with changes we are happy with…all other work will go through a branch, pull request, review, and merge process to minimize conflicts or duplication of work.\nJo is a wizard at grabbing the data we want to work with and saving it in an organized way. She’s cloned the repository to her computer, and the first thing she wants to do is create a fresh branch to work off of, so her changes can be reviewed before being merged into the main branch.\nWe can create a branch in RStudio in one of two ways:\nTools > Terminal > New Terminal and look for the Terminal tab in RStudio. Click on it, and at the prompt, type git checkout -b BRANCH_NAME.\nUsing the Git tab in RStudio and clicking on the New Branch button, type in the branch name, make sure the Remote is set to origin and click Create!\nLet’s create a new branch called jo_download_data. Ideally, each team member will do the same, and each of these branches comes from the main branch, which is the main trunk or “clean” version of the project.\n\n\n\n\n\n\n\nFigure 6: Jo creates a new branch from main to write a data download script in R.\n\n\n\nDownload Data: add, commit, push\nJo’s task is to download the data. Because this is a task that will need to happen regularly, she writes a function to download the specific river discharge data to an organized set of folders. She runs the function to get the most recent data, and saves it locally on her version of the repository!\nAs she works, the general process she follows is:\nadd: Do some work, add some files, scripts, etc. To version control these changes, we need to stage or add that work so it can be versioned by git. We do this with git add <my_file> or check the Staged buttons in the RStudio git panel.\ncommit: Add a commit message that is succinct but descriptive (i.e., added water data files). These are the messages you’ll be able to go back to if you want to travel back in time and see a different version of something…so be kind to your future self and add something helpful.\npush: Finally, when we’re ready to send locally commited changes up the cloud, we push this up to the Github remote cloud repository!\nRemember, you can commit as frequently as you like, and push at the end of the day, or push every time you commit. The timestamp identifier is added with a commit message, not with the push.\n\nYou Try!\n\nTake the following function and:\nMake a new folder in your project called code, save as a script called 01_download_data.R\nstage it (either through the Git tab, or via git add),\ncommit (add a message!),\npush to the jo_download_data branch\n\n\nlibrary(fs)\nlibrary(dataRetrieval)\nlibrary(readr)\nlibrary(glue)\n\n# make a function to download data, defaults to current date for end\n\n# function: dates as \"YYYY-MM-DD\"\nget_daily_flow <- function(gage_no){\n\n  # create folder to save data\n  fs::dir_create(\"data_raw\")\n\n  # set parameters to download data\n  siteNo <- gage_no # The USGS gage number\n  pCode <- \"00060\" # 00060 is discharge parameter code\n\n  # get NWIS daily data: CURRENT YEAR\n  dat <- readNWISdv(siteNumbers = siteNo,\n                    parameterCd = pCode)\n  # add water year\n  dat <- addWaterYear(dat)\n  # rename the columns\n  dat <- renameNWISColumns(dat)\n\n  # save out\n  write_csv(dat,\n            file =\n              glue(\"data_raw/nfa_updated_{Sys.Date()}.csv\"))\n}\n\n# RUN with:\n#siteNo <- \"11427000\"# NF American River\n# get_daily_flow(siteNo)\n\n\n\n\nAnswers!\n\nOnce we’ve made our script and saved it in the code folder, we should see something like this in our Git tab… notice the change when the box is checked.\n\n\n\nNext we want to commit and add a message. Note when we first click the Commit button, we’ll see this screen:\n\n\n\nWhen we enter a commit message and click Commit, we’ll end up with this, which tells us our commit worked…but it’s still only local! Note we’ll have a message saying our branch is\n\nahead of origin/jo_download_data by 1 commit.\n\nThat’s ok, it just means we still need to push our changes up to the remote branch.\n\n\n\nThe final step is to push our changes up to the cloud. Click Push and you should get message back like this. This means things worked…a final check would be go to Github and make sure the file is online!\n\n\n\n\nThink of every commit you make as a train station. The more commits you make (and the more descriptive they are) the easier it will be to get around…particularly because your train can travel back in time!\nPull Requests\nThe next step in this version control odyssey is to get Jo’s work from the jo_download_data branch into the main branch. Here we use a Pull Request. This is a way to review the changes, check for any conflicts (if for instance, folks were working on the same file), and then Merge these changes into the main branch. Then we can delete the jo_download_data branch, create another one to work on the next task, and so on.\nHere’s what we might see on Github if we visited our water-is-life repository after we pushed our changes up. First, we will hopefully see an option to make a pull request because Github recognizes there are changes from another branch that aren’t in the main branch. We want to click the Compare and pull request button.\n\n\n\nNext we have an option to add some additional descriptions, comments, about what this pull request (PR) is doing, and why. We can tag a reviewer (collaborator on the Github repo), and add labels, milestones, etc. These are all helpful for keeping track of what’s done and what’s not.\n\n\n\nAfter we click the green Create pull request button, we should see something that looks like the following. Important! We ideally will see a green checkmark with a message \"This branch has no conflicts with the base branch, which means, our main branch can easily merge this new work in!\nmerge conflicts are not something to be worried about, they happen, and there are a number of ways to resolve them. A good resource and walk through using Github is here, or with command line here.\n\n\n\nExiting vim: if you end up in a text editor and aren’t sure how to exit, you may be in vim. It is the default editor for many systems and programs. To exit without saving changes, hit Esc, then type :q! and hit Enter. To save changes, hit Esc and then type :wq and hit Enter.\n\nGo ahead and click Merge pull request, and wait until you see a screen shortly after that says the Pull request was successfully merged and closed!\n\n\n\n\nfetch and pull\nNow that Jo has completed the first part of the team task, we want to move to the second task, which is cleaning the data. Thankfully Mo is great at data visualization, and has a script that will clean up the code and visualize it for us. But first, Mo needs to pull the changes that Jo just merged into the main repository. There are two approaches to this. One is to use a git fetch, the other is a git pull. The main difference between the two:\nfetch: The safe version, because it downloads any remote content from the repository, but does not update your local repository state. It just keeps a copy of the remote content, leaving any current work intact. To fully integrate the new content, we need to follow a fetch by a merge.\npull: This downloads the remote content and then immediately merges the content with your local state, but if you have pending work, this will create something called a merge conflict, but not to worry, these can be fixed!7.\nMo needs to pull changes in to his branch from main and proceed with the task. Here we assume Jo has already completed work on her branch to download data and clean and transform the data.\n\n\n\n\nYou Try!\n\nTake the following code from Mo and:\npull changes into the repository so everything is up-to-date!\ncreate a unique branch: mo_cleanviz\nsave this code to a new script at code/02_clean_visualize.R\nstage it (either through the Git tab, or via git add),\ncommit (add a message!),\npush to Mo’s branch\nFinish by adding a Pull Request and merging back to main!\n\nAnswers!\n\nNow we have a commit history that might look something like this.\n\n\n\nWhat is one explanation for commit 4? We also have a project that is very easy to re-run and update, and it can be updated or integrated into a parameterized Rmarkdown report which could be shared and updated easily.\n\n\n\n\n\nClosing the loop\nWe’ve reviewed how to create branches, add and revise files, and then commit and push these changes to a shared repository for collaborative projects. Importantly, it’s best practice to always pull/fetch at the beginning of each work session so you have the most-up-to-date changes. And if something needs to be fixed, you can always pull a specific branch and make a change and pull request for that branch before pull requesting and merging back to the main.\nFor example, if Jo needed to make a quick fix to some data analysis, she could pull Mo’s branch, make a fix (commit 4), and merge that back into the same branch before he finalizes the analysis and merges back to the main branch.\n\n\n\nThis is a simplified workflow, but it is very powerful. There are additional options within this workflow, like reverting to previous versions, pruning branches or git history, and more, but the core steps to successfully using version control rely on the material covered above, and practice!\n\nAdditional Git Details\nAlthough there are many more details important to learning version control, many come with more practice and expertise. However, a few important tidbits:\nWhen working on Github, there are options for private and public repositories. private repos are only visible to you and any collaborators you’ve added to your repo. Certain accounts permit unlimited private repos.\n.gitignore: We can access or create this file with the usethis::edit_git_ignore() function. Any file extension or specific file we include in our .gitignore file will mean git ignores it. This is helpful for hidden files or temporary files that may change a lot (i.e., *.html) but aren’t necessary to version contol. You can even ignore entire directories or large files ( >= 100MB). Github is designed to version control scripts, not large amounts of data.\nAdditionally you can use the usethis::git_vaccinate() function to ensure there are no issues or secure files that may get accidentally added to your version control repository.\nLarge files…Github is great, but less ideal for large files. Github will complain with any file over about ~50MB, and anything greater than 100MB requires git-lfs (large file sizes), but it is still unwieldy. See this article for suggestions and options.\n\nResources & Tutorials\nhttps://swcarpentry.github.io/git-novice/\nhttps://learngitbranching.js.org\nR for excel with github\n“Excuse me do you have a moment to talk version control?” by Jenny Bryan\nGithub for project management\n\n\nPrevious module: Project Management Next module: Interactive Visualization\n\nGithub is not the only site that integrates with git to provide cloud-based version control. Other notable examples include Gitlab and Bitbucket. In this module, we will focus on Github, although the underlying git commands you will learn are extensible to other git-based repositories.↩︎\nGit xkcd↩︎\nIf you ever need to edit your global/git profile, you can use a situation report, usethis::git_sitrep() to see what settings exist and how to diagnose problems.↩︎\nsee this excellent overview of the various ways to create/link an RStudio project with a Github repository↩︎\nIt’s a good idea to store all your github repositories in one place (e.g., ~/Documents/Github). We recommend to avoid nesting git/RStudio projects inside other git/RStudio projects–this can create confusion and make it difficult to properly version control different projects. Each project should have its own repository (e.g., ~/Documents/Github/project_01, ~/Documents/Github/project_02).↩︎\nIn more complex workflows, we can pull branches into other branches before pulling them into the main branch. In the module, we will use more simplified examples, but just remember that there are many other ways to branch and merge.↩︎\nhttps://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/resolving-a-merge-conflict-on-github↩︎\n",
      "last_modified": "2021-05-18T19:31:26-07:00"
    }
  ],
  "collections": []
}
