---
title: "Advanced spatial R and mapmaking"
description: | 
   From 1,000 point-clicks to 1 script...
output: 
  distill::distill_article:
    toc: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, purl=FALSE, message=FALSE}

library(knitr)
library(glue)
library(here)
library(purrr)
library(janitor)


```

::: {.obj}
**Learning objectives**

-   Learn to extend and use {sf} for geospatial work
-   Understand the power of script-based geospatial/mapping
-   Expand your geospatial skills in R!
:::


## Overview

The ability to work in one place or with one program from start to finish is powerful and more efficient. By sticking with one single framework or set of tools, we can reduce the mental workload necessary when switch between programs, staying organized in each, and dealing with import/export across multiple programs. While different tools such as ESRI (or ArcPy extensions) are good, they require a paid license and typically still use point-click user interfaces.

The advantage `R` has over these other tools is that it is freely available, provides access to entire statistical/modeling toolboxes, and spatial analysis and mapmaking tools, all while remaining in a single place. 

If we use a functional programming approach (described in previous modules [link]() ) for spatial problems, `R` can be a very robust and powerful tool for analysis and spatial visualization of data! Furthermore, once analyses have been completed, we can re-use the scripts and functions for common spatial tasks (like making maps or exporting specific spatial files).

## Common Geospatial Tasks

There are some common tasks that are often required or repeated, and using a GUI-based approach, these tasks will always require the same number of point and clicks. With a script-based approach, it's much easier to recycle previous code, or just change a variable and re-run the code. This efficiency is magnified immensely when it can be automated or iterated over the same task through time, or multiple data sets.

For example, some common tasks may include:

 - Cropping/creating a specific region or subset of spatial data for different users
 - Making a map with common layers or components but updated data
 - Integrating or spatial joining of datasets
 - Reprojecting spatial data
 
## The power of a script-based analysis with `{sf}`

The `{sf}` package truly makes working with vector-based spatial data easy. We can use a pipeline that includes:

 - `st_read`: read spatial data in (e.g., shapefiles)
 - `st_transform`: transform or reproject data
 - `st_buffer`: buffer around data
 - `st_union`: combine data into one layer
 - `st_intersection`: crop or intersect one data by another
 - `group_split` & `st_write` to split data by a column or attribute and write out
 
There are many more options that can be added or subtracted from these pieces, but at the core, we can use this very functional approach to provide data, make maps, conduct analysis, and so much more.

## A Climate/Hydrology Example

Let's use an example where we take global climate data and crop it down to just an area of interest and then join it to additional data and write it back out a few counties of interest in California (Yolo, Sacramento, San Joaquin, Sutter).

### The Packages

```{r libraries}

library(here)
library(sf)
library(dplyr)
library(readr)
library(viridis)
library(ggplot2)
library(USAboundaries)
library(rnaturalearth)
library(GSODR)
library(ggrepel)
library(cowplot)
library(mapview)      # interactive maps!
mapviewOptions(fgb = FALSE)

```

### Get State & County Data

First we want to get state and county boundaries. The {USAboundaries} package is excellent for this.

```{r boundaries}

# get USA states, filter out Puerto Rico, Alaska, and Hawaii for now
us <- USAboundaries::us_boundaries(type="state", resolution = "low") %>% 
  filter(!state_abbr %in% c("PR", "AK", "HI"))

# get CA boundary with high definition
ca <- USAboundaries::us_states(resolution = "high", states = "CA")

# make a box around CA (a grid with an n=1) for inset
ca_box <- st_make_grid(ca, n = 1)

# get CA county boundary
ca_co <- USAboundaries::us_counties(resolution = "high", states = "CA")

# make sure we have all the pieces with a quick test plot
plot(us$geometry)
plot(ca$geometry, add=T, col="gray50", border="maroon")
plot(ca_co$geometry, add=T, border="pink", col=NA)
plot(ca_box, add=T, border="red3", col=NA, lwd=2)

```

### Get Climate Data from the [GSODR](https://ropensci.github.io/GSODR/)

Next let's take historical global climate data stations (Global Surface Summary of the Day=GSOD^[ For more about the GSOD data, see NOAA's site [here](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ncdc:C00516)]) and filter to stations that are just in California.

```{r}

# load the isd_history file:
load(system.file("extdata", "isd_history.rda", package = "GSODR"))

# make spatial
isd_history <- as.data.frame(isd_history) %>% 
  st_as_sf(coords=c("LON","LAT"), crs=4326, remove=FALSE)  

# filter to US and CA, many sites out in buoys along coast
isd_history_ca <- dplyr::filter(isd_history, CTRY=="US", STATE=="CA")

```

Now let's plot all the stations globally and see what we have. Here we can leverage the many points to actually help fill out our global outlines a bit (there are `r nrow(isd_history)` stations to work with!).

```{r, layout="l-page"}

# view!
library(rnaturalearth)
library(rnaturalearthdata)

# get countries
world <- ne_countries(scale = "medium", returnclass = "sf")

# plot
plot(isd_history$geometry, pch=16, cex=0.2, col="gray50")
plot(world$geometry, add=T, border = "gray10")
plot(ca$geometry, col="maroon", add=TRUE)
title("GSOD Climate Stations")

```


## Filter Data

Next we can zoom into just California. There are `r nrow(isd_history_ca)` total stations we can work with, but not all counties have stations.

```{r, layout="l-page"}

# look at CA sites only
plot(ca$geometry, col=alpha("gray", 0.5), border="#440154FF", lwd=1.5)
plot(ca_co$geometry, add=T, border="purple", col=NA)
plot(isd_history_ca$geometry, add=T, pch=21, bg="#21908CFF", cex=0.7, col="black")
title("GSOD Climate Stations labeled as CA")
```



## Filters & Spatial Joins

Ok! Let's quickly filter down to the counties of interest (Stanislaus, San Joaquin, Sacramento, and Sutter), and select a single station from each county that has a station.

First we filter to 4 specific counties, then join to the climate data stations. Note, we are using a spatial join here, which gets polygons that contain points. We can also use an anti_join (the `!`) to find counties that contain *no* stations. These operations can be helpful when exploring and understanding a dataset, to identify gaps, highlight specific areas, etc.

```{r, layout="l-page"}

# get specific counties
ca_co_select <- ca_co %>% filter(name %in% 
                                   c("Sacramento",
                                     "San Joaquin", 
                                     "Sutter"))

# check CRS is the same
identical(st_crs(ca_co_select)$epsg, st_crs(isd_history)$epsg)

# SPATIAL JOIN: Get CA county POLYGONS that contain ISD points 
# does not bring attributes from points forward
ca_co_isd_poly <- ca_co_select[isd_history_ca, ]

# anti_join: find counties that don't contain ISD points
ca_co_poly_anti <- ca_co[!lengths(st_intersects(ca_co, isd_history_ca)), ]

# plot of all CA counties with and without ISD stations
plot(ca_co$geometry, col=alpha("gray70",0.3))
#plot(ca_co_select$geometry, col=alpha("gray",0.3))
plot(ca_co_poly_anti$geometry, col=alpha("purple",0.3), add=TRUE)
plot(ca_co_isd_poly$geometry, col=alpha("forestgreen",0.6), add=TRUE)
plot(isd_history_ca$geometry, add=T, pch=21, bg="yellow", cex=0.8, col="black")
title("Counties with GSOD ISD Stations (green)\n No Stations (purple)")
```

Ok, so there are 2 counties from our list that have GSOD stations. Let's find out some more info on the specific stations from our counties of interest, and pull one station per county. In this case, let's pull the station that has the most data (or longest period of time), so we can join these data to the nearest groundwater station.

## Select GSOD Stations for Selected Counties

Here we use `st_join` directly to filter points that fall within polygons. Here, that's the GSOD stations that fall within our selected counties. 

<aside>
When we have 2 `{sf}` dataframes, we can also spatially join using the `[ ]` notation as we did in the county joins above. `st_join()` and `[ ]` are equivalent when spatial data shares the same CRS.
</aside>

```{r}

isd_select <- st_join(isd_history_ca, ca_co_isd_poly, left=FALSE)

mapview(isd_select, zcol="name", layer.name="County") +
  mapview(ca_co_isd_poly, alpha=0.5, legend=FALSE)

```

### Writing Spatial Data Out

We may want to save these data and send to colleagues before we proceed with further analysis. As we've shown before, functional programming is a great option, and we can create a function to split data and write it out for future use, or to share and distribute. Here we are using a fairly simple example, but the concept can be expanded.

Let's use the `{purrr}` package to iterate over two different lists and write them to a geopackage (a self contained spatial database). Geopackages are a great way to save vector-based spatial data, they can be read by ArcGIS and spatial software, and they are compact and self-contained (unlike shapefiles).

```{r, echo=TRUE, eval=FALSE}
library(purrr)
library(glue)
library(janitor)

# first split ISD data by county:
isd_select_split <- isd_select %>% 
  rename(cnty_name=name) %>% # avoid duplicate names
  split(.$cnty_name) # split by cnty name

# split county data by county
ca_co_select_split <- ca_co_isd_poly %>% 
  rename(cnty_name=name) %>% # avoid duplicate names
  split(.$cnty_name) # split by cnty name

# make a list of clean names with janitor pkg
(clean_cnty_names <- make_clean_names(ca_co_isd_poly$name))

# now apply function to write out points by county
map2(isd_select_split, # list of points
     clean_cnty_names, #list of cnty names
     ~st_write(.x, 
               dsn = glue("data/{.y}_gsod.gpkg"),
               layer = glue("{.y}_isd_pts")))

# to add additional layers we can do the same with a diff layer
map2(ca_co_select_split, # list of points
     clean_cnty_names, #list of cnty names
     ~st_write(.x, 
               dsn = glue("data/{.y}_gsod.gpkg"),
               layer = glue("{.y}_isd_cnty")))

```

```{r, include=FALSE}
(clean_cnty_names <- make_clean_names(ca_co_isd_poly$name))

```


To make sure this worked as intended, we can check what layers exist in each of these geopackages.

```{r}

# check layers in one of gpkg files:
map(c("sacramento", "san_joaquin"), 
    ~st_layers(glue("data/{.x}_gsod.gpkg")))

```

<!--

formative quiz here: mult choice with other options to place on line 269:
 - clean_cnty_names
 - ca_co_isd_poly
 - ca_co$name
-->

## Filter Data

Now we have a subset of stations to use, let's filter these down to one per county for now. Let's use the station that has the longest period of record, and we can then pair this with the nearest groundwater station.

```{r}

library(lubridate)

# calculate the duration of data for each station
isd_stations <- isd_select %>% 
  mutate(date_begin = ymd(BEGIN),
         date_end = ymd(END), 
         tot_yrs = interval(date_begin, date_end) %>% 
           time_length("year") %>% round(1))

# group by county name
isd_stations_keep <- isd_stations %>% group_by(name) %>% 
  filter(tot_yrs > 1) %>% 
  # get the station with the longest set of data
  filter(tot_yrs == max(tot_yrs)) %>% 
  select(STNID:LON, tot_yrs) %>% 
  ungroup

glimpse(isd_stations_keep)

```

## Buffer Data

Now we have stations with the longest period of record in each county, but let's see if we can buffer from these to find the nearest groundwater stations (within a kilometer) of the GSOD station.

To buffer our data, it's best if we use a projection that isn't latitude/longitude, so we can specify things in units that are easier to understand (kilometers or miles) instead of arc degrees.

```{r}

# transform our points
isd_stations_keep <- st_transform(isd_stations_keep, 3310)

# now buffer 5km around each station as our window to look within
isd_stations_buff_5km <- st_buffer(isd_stations_keep, dist = 5000) %>% 
  # transform back to proj of other components
  st_transform(., st_crs(ca_co_isd_poly))

isd_stations_keep <- st_transform(isd_stations_keep, st_crs(ca_co_isd_poly))

# double check on map
mapview(ca_co_isd_poly, col.regions="forestgreen", 
        alpha.regions=0.2, layer.name="Selected Counties") +
mapview(isd_stations_buff_5km, col.regions="orange", 
        alpha.regions=0.5, color="orange", layer.name="5km Buffer") +
  mapview(isd_stations_keep, col.regions="yellow", cex=1.5,
          layer.name="ISD Stations")
```

## Get Groundwater Stations and Join to Nearest

Next we want to see what the nearest groundwater station may be for each of our climate stations. Let's use the stations dataset that's been used in previous modules.

```{r}

gw_stations <- read_csv(here("data", "gwl", "stations.csv")) %>% 
  # make a clean county name col to use
  mutate(COUNTY_NAME_CLEAN = snakecase::to_snake_case(COUNTY_NAME))

dim(gw_stations) # many stations!

# filter to just our counties of interest
gw_stations_filt <- gw_stations %>% 
  filter(COUNTY_NAME_CLEAN %in% clean_cnty_names)

# convert stations into an sf object by specifying coordinates and crs
gw_stations_filt <- st_as_sf(gw_stations_filt, 
                     coords = c("LONGITUDE", "LATITUDE"), # lon first
                     crs = 4326, # match other data
                     remove = FALSE) 

# map
mapview(ca_co_isd_poly, col.regions="forestgreen", alpha.regions=0.2,
        layer.name="Selected Counties") + 
  mapview(gw_stations_filt, cex=1.5, layer.name="GW Stations",
          alpha.regions=0.2) +
  mapview(isd_stations_buff_5km, col.regions="orange", 
        alpha.regions=0.5, color="orange", layer.name="5km Buffer")
```

Now we can join these data and see what data (if any) fall into our 5km buffers. Here we can use the `st_intersection` function.

```{r}

# this takes a second!
gw_stations_select <- st_intersection(gw_stations_filt, isd_stations_buff_5km)

# how many per county?
gw_stations_select %>% st_drop_geometry() %>% 
  group_by(COUNTY_NAME) %>% tally()

```

<aside>
Why did we use `st_drop_geometry()`? Sometimes it's cleaner (and faster) to operate on the `data.frame` without any of the spatial data. Especially when we have many hundreds or thousands of complex spatial data.
</aside>

## Get GW Data

Great, so there are a good number of stations to work with. Let's filter the groundwater data to find the station with the best long running dataset we can pair with the climate data.

```{r}
# get data (it's large, but can read straight from zip!)
gw_data <- read_csv(here("data", "gwl", "measurements.csv.zip"))

# filter to our selected stations
gw_data_filt <- gw_data %>% 
  filter(SITE_CODE %in% gw_stations_select$SITE_CODE) %>% 
  # join station metadata in
  left_join(., gw_stations_select)

# now look at how many records exist for depth to groundwater,
# by county and station:
gw_data_filt %>% group_by(COUNTY_NAME, SITE_CODE) %>% 
  filter(!is.na(GSE_WSE)) %>% tally() %>% 
  filter(n>20) %>% # keep stations with more than 20 observations 
  DT::datatable()

# make a list of these station ids
gw_data_filt %>% group_by(COUNTY_NAME, SITE_CODE) %>% 
  filter(!is.na(GSE_WSE)) %>% tally() %>% 
  filter(n>100) %>% 
  pull(SITE_CODE) -> gw_stations_w_data

```


```{r}
# filter to our stations of interest
gw_long_data <- gw_data_filt %>% 
  filter(SITE_CODE %in% gw_stations_w_data) %>% 
  # join station metadata in
  left_join(., gw_stations_select)

```

### Visualize!

Let's take these data and visualize them. It's always helpful to visually inspect things

```{r}
gw_long_data %>% 
  ggplot(aes(x=MSMT_DATE, y=GSE_WSE, group = SITE_CODE, color=COUNTY_NAME)) +
  geom_line(alpha = 0.5, show.legend=FALSE) +
  theme_classic() + 
  facet_grid(~COUNTY_NAME, scales = "free")

```

## Snap to the Nearest Point

The final filter involves snapping our ISD station points (n=2) to the nearest groundwater station from the stations we filtered to above. We can then use these data to generate some analysis and exploratory plots.

Snapping spatial data can be tricky, mainly because decimal precision can cause problems. One solution is to add a slight buffer around points or lines to ensure successful pairing.

For this example, we'll use `st_nearest_feature`, which gives us an index of the nearest feature (row) between two sets of spatial data. In this case, we have two sets of points.

```{r}

# make the gw_select_filt spatial
gw_stations_select_filt <- gw_stations_select %>% 
  filter(SITE_CODE %in% gw_long_data$SITE_CODE)

# get row index of nearest feature between points:
gw_nearest_index <- st_nearest_feature(isd_stations_keep, gw_stations_select_filt)

# now filter using the row index
gw_stations_final <- gw_stations_select_filt[gw_nearest_index,]

# now plot!
mapview(gw_stations_select, cex=0.75, col.regions="gray",
        layer.name="GW Stations")+ 
  mapview(gw_stations_final, col.regions="yellow",
          layer.name="Nearest GW Station to ISD Pt") +
  mapview(isd_stations_keep, col.regions="forestgreen",
          layer.name="ISD Stations") + 
  mapview(isd_stations_buff_5km, col.regions="orange", 
        alpha.regions=0.5, color="orange", legend=FALSE)

```


### Add Distance Data

If we want to calculate the distance between each feature, we can use `st_distance` and filter/sort based on these data.

```{r}

sf_use_s2(FALSE) # deal with new s2 (spherical geometry calculations)

# get vector of distances from each gw station to the nearest isd station
dist_to_isd <- st_distance(isd_stations_keep, gw_stations_final, by_element=TRUE)

# use units package to convert units to miles or km
(dist_to_isd_mi <- units::set_units(dist_to_isd, miles))
(dist_to_isd_km <- units::set_units(dist_to_isd, km))

# bind back to final dataset:
gw_stations_final <- gw_stations_final %>% 
  cbind(dist_to_isd_mi, dist_to_isd_km)

```


## Download GSOD Data

We are nearly done! Finally we can pull climate data for our stations of interest and join these data with our groundwater data. We'll add these data to our geopackage databases for each respective county to keep our data all together in one place.

```{r}

# get data (this takes a hot minute):
climdata <- get_GSOD(station = isd_stations_keep$STNID, years = c(1960:2020))

```

```{r, include=FALSE}
# write out for future use since it takes a good minute
save(climdata, file = here("data", "isd_climdata_3_stations.rda"))
```


Data is large (>40,000 observations), so let's write it out to the geopackage first (in case something breaks later).

```{r}

# to add additional layers we can do the same with a diff layer
climdata %>% split(.$STNID) %>% 
  map2(., clean_cnty_names,
       ~st_write(.x, 
                 dsn = glue("data/{.y}_gsod.gpkg"),
                 layer = glue("{.y}_isd_data"), delete_layer=TRUE))

# check layers
map(clean_cnty_names, ~st_layers(glue("data/{.x}_gsod.gpkg")))

```

### Visualize!

First filter out data down to our selected stations.

```{r}
# filter to our stations of interest
gw_data_out <- gw_data_filt %>% 
  filter(SITE_CODE %in% gw_stations_final$SITE_CODE) 

```

Let's take these data and visualize them. It's always helpful to visually inspect things

```{r}
gw_data_out %>% 
  ggplot(aes(x=MSMT_DATE, y=GSE_WSE, group = SITE_CODE, color=COUNTY_NAME)) +
  geom_line(alpha = 0.5, show.legend=FALSE) +
  theme_classic() + 
  facet_wrap(~COUNTY_NAME)

```


 - plot groundwater vs. temperature (or something like that)


Use GSOD lesson and show climate stations with climate data, write out by county: https://ryanpeek.org/mapping-in-R-workshop/03_spatial_joins.html

You want to find the nearest river/WQ stations to a groundwater area of interest?

Spatial joins, st_nearest, buffering
Why click button GIS tasks may be easier to integrate with R? What if you needed to do this for each county or river?
End up with a list of stations/points that we can map with the buffer

How to pull spatial river data or other spatial data via R (nhdtoolsPlus package, dataRetrieval)
