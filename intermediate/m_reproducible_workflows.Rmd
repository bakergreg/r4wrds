---
title: "Reproducible workflows and automation"
description: | 
   How to make computers do your bidding  
output: 
  distill::distill_article:
    toc: true
---

```{r setup, include=FALSE, purl=FALSE, message=FALSE}

library(knitr)
library(glue)
library(here)

```

::: {.obj}
**Learning objectives**

-   Practice functional programming iterative workflows to power reproducible analyses
-   Understand how to use nested lists within dataframes
-   Practice conditional execution
-   Discuss task schedulers and strategies to fully automate workflows\
:::

<br>

## Reproducible workflows and automation

In this module, we extend lessons in functional programming from the previous [module on iteration](m_iteration.html), as well as best practices discussed in the [project management module](m_project_management.html) to approach a case study. Specal attention is paid to automation.

As you expand your ability to write `R` code, **automation** becomes possible. "Automation" can mean different things, but generally refers to the process of replacing time that you would spend on a workflow with a computer program, or an `R` script in the context of this course. The beauty of automation is that you can create workflows that would be nearly impossible to complete in a human lifetime, and run them thousands of times, all while you're out on a coffee break. In some circles, this is sometimes jokingly referred to as, "making computers do your bidding." Automation happens whenever you write a program that saves you time, freeing you to do other things.

The very act of writing an `R` script (or program) to perform analyses is automation, because you have automated the need to point and click through various tasks and type instructions at particular times in a traditional point-and-click workflow.

However, the `R` program still needs a human to run it, and perhaps you have not one, but a chain of 5 scripts that are evaluated sequentially to import, clean, model, visualize, and output data. Someone needs to run the program. Another step up the automation ladder is to wrap these 5 scripts in a single program that calls them all in sequence, like a master switch. Instead of running 5 scripts, now we only need to run 1.

There is still automation potential. We can write another program to call the "master switch" at specified times, fully remove ourselves from the pipeline, and rest assured that our program will run all on its own (although it may require maintenance from time to time).

(ref:automate-fig) *Automation is like watering a row of plants. Each program is like a bucket that waters one plant at a time. Wrapping the programs into a single "master switch" control script is like installing drip irrigation so that all plants are watered at once. In order to fully automate, we can set a task scheduler, which is like adding a timer onto our drip irrigation system.*

```{r automate-fig, eval=TRUE, echo=FALSE, out.width='100%', fig.cap='(ref:automate-fig)'}
knitr::include_graphics(here("images", "automation.png"))
```

We should always strive for at least a degree of automation represented by the left-most figure, with a set of programs for each step in our workflow, and functions abstracted out of these programs[^1] to improve readability and project organization.

[^1]: As discussed in the project management module, [abstracting functions from code](m_project_management.html#abstracting-functions-from-code) is a best practice of defining functions and storing them in separate scripts to keep your workspace easier to read and de-bug.

Whether or not to write wrapper programs or configure task schedulers depends on the needs of the workflow. Guidance on when and when to not automate will be discussed in this module.

To demonstrate varying degrees of automation in a real-life example and extend our practice of functional programming, we will focus on a case study using the DWR's Periodic Groundwater Level Database.

<br>

## Case Study: groundwater level trend analysis

The California Department of Water Resources publishes a routine report on groundwater level conditions that shows groundwater level trends on a point-by-point basis. Short-term (i.e., 1, 3, and 5 year) and long-term trends (i.e., 10 and 20 years) are displayed in the report.

(ref:ca-dwr-fig) *The ["California Groundwater Conditions Update - Spring 2020", DWR (2020)](https://water.ca.gov/-/media/DWR-Website/Web-Pages/Programs/Groundwater-Management/Data-and-Tools/Files/Maps/Groundwater-Level-Change/DOTMAP_Reports/Spring-2020-Groundwater-DOTMAP-Report.pdf) demonstrates the rate and direction of groundwater level change over different time periods, shown here is the 20000-2020 trend.*

```{r ca-dwr, eval=TRUE, echo=FALSE, fig.cap='(ref:ca-dwr-fig)'}
knitr::include_graphics(here("images", "dwr_gwl.png"))
```

Let's imagine that our task is to assess change in groundwater level across all stations and plot it on a map, just as we see in the report. In the sections that follow we will calculate groundwater level change trends at representative monitoring points in the [Periodic groundwater level database](https://data.cnra.ca.gov/dataset/periodic-groundwater-level-measurements) we have been using in this course, and demonstrate best practices in creating a reproducible, automatable pipeline. Along the way, we will also cover key concepts including conditional execution, functional programming with nested lists, tips for writing helpful functions, task scheduling, and logging.

To begin, let's import data and join it. If your internet connection is slow, use `d <- read_csv(here("data", "gwl_sac.csv"))` to skip the step below and read in the data. If you want to see how to download this data, follow along with the code below[^2]:

[^2]: Note that over time, the URLS in this code may change, and the data will certainty grow in volume. Thus, when comparing the output of the code to the data provided with this course, there may be differences because this code always grabs the most up-to-date data, which will probably contain more records than what we provide.

```{r import, eval = FALSE}
library(tidyverse)
library(here)

# urls for data
base_url <- 
  "https://data.cnra.ca.gov/dataset/dd9b15f5-6d08-4d8c-bace-37dc761a9c08/resource/"
urls <- paste0(
  base_url, 
  c("af157380-fb42-4abf-b72a-6f9f98868077/download/stations.csv",
    "bfa9f262-24a1-45bd-8dc8-138bc8107266/download/measurements.csv",
    "f1deaa6d-2cb5-4052-a73f-08a69f26b750/download/perforations.csv")
)

# create directory and define paths to save downloaded files
dir.create(here("data", "pgwl"))
files_out <- here("data", "pgwl", basename(urls))

# download files
walk2(urls, files_out, ~download.file(.x, .y))

# read and combine - use datatable for speed and memory management
d <- map(files_out, ~data.table::fread(.x)) %>% 
  reduce(left_join, by = "SITE_CODE")

# d contains ~2.3 million observations: filter to Sacramento County
d <- filter(d, COUNTY_NAME == "Sacramento")
```

<aside>

<br>

We use copied URLs here, but for large, standardized websites you can use [`{rvest}` and SelectorGadget](https://rvest.tidyverse.org/articles/articles/selectorgadget.html) to automate scraping URLs and other data. We also used `purrr::reduce()` to recursively `left_join()` a list of dataframes. This is a powerful concept, because the alternative is to manually `left_join()` each list element. Read up on `?reduce` to learn more.

<br>

</aside>

```{r, read-sac-co-gwl, echo = FALSE, message=FALSE, error=FALSE}
library(tidyverse)
library(here)
d <- read_csv(here("data", "gwl", "gwl_sac.csv"))
```

Now that we have our Sacramento data in `R` either by downloading it with the code above, or by reading it in with `d <- read_csv(here("data", "gwl_sac.csv"))`, let's transform it to an `sf` object and plot just to sanity check that we're indeed looking at Sacramento County groundwater levels.

```{r gwl-mapview, layout = "l-page"}
library(sf)
library(mapview)
mapviewOptions(fgb = FALSE)

# convert to sf object
d <- st_as_sf(d, coords = c("LONGITUDE", "LATITUDE"), crs = 4269) 

# take the first observation per SITE_CODE since they all have 
# the same location, and plot
group_by(d, SITE_CODE) %>% 
  slice(1) %>% 
  ungroup() %>% 
  mapview(popup = NULL)
```

### Nested lists

Recall that we want to assess change in groundwater level across all stations and plot it on a map. In other words, assume we have the linear model describing a set of groundwater elevations ($\hat{Y}_i$), as a function of time ($X_i$), proportional to some unknown y-intercept ($\beta_0$) and coefficient ($\beta_1$) for a set of observations at stations $i = 1, ..., n$:

$$
\hat{Y}_i = \beta_0 + \beta_1(X_i) + \epsilon_i
$$

Time alone is actually a poor predictor of groundwater level, but for our purposes, the linear model above is useful because the $\beta_1$ coefficient can be interpreted as the average groundwater level trend at a monitoring site over some time frame. If $\beta_1 > 0$, the average groundwater level is increasing, and if $\beta_1 < 0$, it is decreasing. The magnitude of $\beta_1$ tells us by how much the groundwater level is increasing or decreasing and can be plotted per monitoring site, and compared to other monitoring sites to understand the spatial distribution of groundwater level change in a region. In this example, our region of interest is Sacramento County, but as we will demonstrate, we will create a function that allows us to apply this workflow to any county, or even the entire state.  

To apply a linear model to each monitoring site, we'll use the `lm()` function in base `R`. If we apply the function to our entire dataset, we'll end up with one linear model of all groundwater levels in Sacramento county:

```{r gwl-1}
ggplot(d) +
  geom_line(aes(MSMT_DATE, WSE, group = SITE_CODE), alpha = 0.5) +
  geom_smooth(aes(MSMT_DATE, WSE), method = "lm", se = FALSE)
```

Let's filter out that one outlier (groundwater levels don't just spike by hundreds of feet so rapidly like that), filter the date range of the data to the last 20 years, and replot. We can see a slight negative groundwater level trend over the basin, but this spatial average may be skewed by monitoring stations with more data than others and increased data density in more recent years. 

```{r gwl-2}
# recent data without the obviously erroneous outlier
d_recent <- filter(d, WSE >-200 & MSMT_DATE >= lubridate::ymd("2000-01-01"))

ggplot(d_recent) +
  geom_line(aes(MSMT_DATE, WSE, group = SITE_CODE), alpha = 0.5) +
  geom_smooth(aes(MSMT_DATE, WSE), method = "lm", se = FALSE)
```

What we need is a linear model for each and every one of the individual stations in this dataset. For example, here are a few of the monitoring sites.

```{r gwl-3}
d_recent %>% 
  filter(SITE_CODE %in% unique(d$SITE_CODE)[1:7]) %>% 
  ggplot() +
  geom_line(aes(MSMT_DATE, WSE)) +
  geom_smooth(aes(MSMT_DATE, WSE), method = "lm", se = FALSE) +
  facet_wrap(~SITE_CODE)
```

`{ggplot2}` is great for visualizing what we are looking for. Now, it's time to calculate those numbers with `lm()`. We will use `{broom}` to tidy up model results in a clean dataframe. Note that because the class of our `MSMT_DATE` column is `"POSIXct"`, the units returned for our coefficient $\beta_1$ will be in $ft/sec$ rather than the the more intuitive $ft/yr$, thus we define a function to handle this conversion. For the entire dataset the average annual change in groundwater level over the 20 year timeframe is:

```{r gwl-sac}
# convert from seconds to years
sec_to_yr <- function(sec){
  yr <- sec * 31557600
  return(yr)
}

# create the linear model and view output
m <- lm(WSE ~ MSMT_DATE, data = d_recent) 

m

# tidy the model output with broom and convert ft/sec to ft/yr
broom::tidy(m) %>% 
  slice(2) %>% 
  mutate(estimate_ft_yr = sec_to_yr(estimate))
```

From above, we see that the average annual decline is `r round(sec_to_yr(coefficients(m)["MSMT_DATE"]), 2)` $ft/yr$.  

We could turn this linear model call into a function and apply over a list of dataframes created by `split(d, d$SITE_CODE)` (or the tidyverse equivalent, `group_split(d, SITE_CODE)`), but there's a better way with functional programming.

We will use nested lists, specifically with the `tidyr::nest()` function, which allows us to convert a dataframe grouped by some variable (in our case it will be `SITE_CODE`) into a dataframe with a column that "nests" the data for each unique grouping variable. For instance:

```{r nest}

```



Plot beta_1. How we can get this with iteration. Begin with one linear model on one station. Then use lists. Introduce: Nested lists (explain list-columns) (linear models - slope of many dataframes)

### How to write helpful functions

If you copy paste a workflow more than 3 times, write a function reproducible workflows

Keeping other users in mind, using `stop()` and `cat()` messages.

args: start date, end date (for the filter pre-lm), trim_outliers (to illustrate conditional execution)

-   make one plot, then function

-   write one data set, then function for it all

imagemagik the plots into a gif

safely(), possibly() - toy cases to demo these functions

### Conditional execution

If statements within a function

Example for transforming spatial data to the same projection if the crs differs. First plot different crs, and then abstract it into a code chunk with a variable for the default crs used in a project.

-   maybe later use it to trim outliers with arg `trim_outliers` and `if isTRUE(trim_outlies){trim 'em}`

-   another if statement for argument `county`, when, if NULL, does the operation on th entire dataset.

### When to define a pipeline

Analogy of watering a garden of plants one by one, or having one drip irrigation line.

How and when to use `source(my_script.R)` to abstract out functions and other preprocessing pipelines to simplify scripts. The idea of scripts that build on one another in an increasingly complex project.

Take example above and make a control.R - to be used later with task scheduler or cron - we know this data updates monthly, so set it there. Imagine for flow data, this can be every day! Data frequency, latency.

### Task scheduling

Here's how to use task scheduler and cron

### Logging

When things are running without your supervision, things may break so it's a good idea to print information whenever the job kicks off so you can review this information in case something doesn't go as planned. This is known as **logging**, which is cornerstone in software engineering, but less so in data science. Logging becomes important for larger jobs with potential to break, or when inputs may sufficiently change in such a way that breaks code.

<br>

## When to not automate

As a data scientist, you'll often need to perform once-over ad-hoc analyses. You may or may not need to return to these analyses, or components of the analyses. Automation takes time, and just because we have the tools to automate doesn't mean that we should. Whether or not you should automate depends on how often you anticipate the workflow needs to be re-evaluated. The more the workflow needs to be run, the more time we can justify spending on firming up the reproducibility and automation potential of program. When it's not immediately apparant that automation is necessary (e.g., "a daily report of streamflow automatically scraped from 1,000 sensors") a general rule of thumb for whether or not to automate may boil down to the following considerataion:

> Will the time saved in the future by automation exceed the time spent automating the process now? If so, automate.

(ref:automate-xkcd2-fig) *Figure caption TBD*

```{r time-fig, eval=TRUE, echo=FALSE, out.width='75%',fig.cap='(ref:automate-xkcd2-fig)'}
knitr::include_graphics(here("images", "is_it_worth_the_time.png"))
```

Don't underestimate the time it can take to automate either.

(ref:automate-xkcd-fig) *Figure caption TBD*

```{r no-automate-fig, eval=TRUE, echo=FALSE, out.width='75%',fig.cap='(ref:automate-xkcd-fig)'}
knitr::include_graphics(here("images", "automation_xkcd.png"))
```

<br>

## Code as instructions

We've all done a point-and-click analysis before that becomes unwieldy, large, and "too big to fail." In other words, we cross our fingers and hope we'll never have to repeat the analysis because either:

1.  it took so long to complete that it would be unimaginable to spend that same amount of time repeating it\
2.  certain parts of the workflow were subjective, so repeating it would yield a different result\
3.  repeating one part of the workflow would invalidate downstream components and derivative products of the workflow, creating even more work

Code is best thought of as a set of instructions. No matter the language the code is in, whether it's `R`, `Python`, or another language for data science, at its most fundamental level, the code represents a linear set of instructions to carry out on a computer. Coding is the automation of routine point-and-click tasks, and a good program (e.g., `.R` script) will replace the need to click thrugh an analysis. Moreover, as data volume (size) and frequency (timing) increase, many point-and-click software tools are stretched to their limits and become slow or non-responsive, but lower-level code still works, and usually works much faster.

<br>

## Additional resources

-   cron blog posst
-   windows task sceduler link
-   something from efficient R?

<br>

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

<a href="m_iteration.html" class="btn btn-secondary" style="float: left">Previous module:<br>Iteration</a> <a href="m_parameterized_reports.html" class="btn btn-secondary" style="float: right;">Next module:<br>Parameterized reports</a>
