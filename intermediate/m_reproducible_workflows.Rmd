---
title: "Reproducible workflows and automation"
description: | 
   How to make computers do your bidding  
output: 
  distill::distill_article:
    toc: true
---

```{r setup, include=FALSE, purl=FALSE, message=FALSE}

library(knitr)
library(glue)
library(here)

```

::: {.obj}
**Learning objectives**

-   Practice functional programming iterative workflows to power reproducible analyses
-   Understand how to use nested lists within dataframes
-   Practice conditional execution
-   Discuss task schedulers and strategies to fully automate workflows  
:::


<br>

## Reproducible workflows and automation

In this module, we extend lessons in functional programming from the previous [module on iteration](m_iteration.html), as well as best practices discussed in the [project management module](m_project_management.html) to approach a case study. Specal attention is paid to automation. 

As you expand your ability to write `R` code, **automation** becomes possible. "Automation" can mean different things, but generally refers to the process of replacing time that you would spend on a workflow with a computer program, or an `R` script in the context of this course. The beauty of automation is that you can create workflows that would be nearly impossible to complete in a human lifetime, and run them thousands of times, all while you're out on a coffee break. In some circles, this is sometimes jokingly referred to as, "making computers do your bidding." Automation happens whenever you write a program that saves you time, freeing you to do other things.  

The very act of writing an `R` script (or program) to perform analyses is automation, because you have automated the need to point and click through various tasks and type instructions at particular times in a traditional point-and-click workflow. 

However, the `R` program still needs a human to run it, and perhaps you have not one, but a chain of 5 scripts that are evaluated sequentially to import, clean, model, visualize, and output data. Someone needs to run the program. Another step up the automation ladder is to wrap these 5 scripts in a single program that calls them all in sequence, like a master switch. Instead of running 5 scripts, now we only need to run 1. 

There is still automation potential. We can write another program to call the "master switch" at specified times, fully remove ourselves from the pipeline, and rest assured that our program will run all on its own (although it may require maintenance from time to time).

(ref:automate-fig) *Automation is like watering a row of plants. Each program is like a bucket that waters one plant at a time. Wrapping the programs into a single "master switch" control script is like installing drip irrigation so that all plants are watered at once. In order to fully automate, we can set a task scheduler, which is like adding a timer onto our drip irrigation system.*

```{r automate-fig, eval=TRUE, echo=FALSE, out.width='100%', fig.cap='(ref:automate-fig)'}
knitr::include_graphics(here("images", "automation.png"))
```


We should always strive for at least a degree of automation represented by the left-most figure, with a set of programs for each step in our workflow, and functions abstracted out of these programs[^1] to improve readability and project organization. 

[^1]: As discussed in the project management module, [abstracting functions from code](m_project_management.html#abstracting-functions-from-code) is a best practice of defining functions and storing them in separate scripts to keep your workspace easier to read and de-bug.  

Whether or not to write wrapper programs or configure task schedulers depends on the needs of the workflow. Guidance on when and when to not automate will be discussed in this module. 

To demonstrate varying degrees of automation in a real-life example and extend our practice of functional programming, we will focus on a case study using the DWR's Periodic Groundwater Level Database.


<br>

## Case Study: groundwater level trend analysis

The California Department of Water Resources publishes a routine report on groundwater level conditions that shows groundwater level trends on a point-by-point basis. Short-term (i.e., 1, 3, and 5 year) and long-term trends (i.e., 10 and 20 years) are displayed in the report. 


(ref:ca-dwr-fig) *The ["California Groundwater Conditions Update - Spring 2020", DWR (2020)](https://water.ca.gov/-/media/DWR-Website/Web-Pages/Programs/Groundwater-Management/Data-and-Tools/Files/Maps/Groundwater-Level-Change/DOTMAP_Reports/Spring-2020-Groundwater-DOTMAP-Report.pdf) demonstrates the rate and direction of groundwater level change over different time periods, shown here is the 20000-2020 trend.*

```{r ca-dwr, eval=TRUE, echo=FALSE, fig.cap='(ref:ca-dwr-fig)'}
knitr::include_graphics(here("images", "dwr_gwl.png"))
```

Let's imagine that our task is to assess change in groundwater level across all stations and plot it on a map, just as we see in the report. In the sections that follow we will do exactly that, and demonstrate best practices in creating a reproducible, automatable pipeline, along the way covering key concepts including xyz.


### Conditional execution

If statements within a function

Example for transforming spatial data to the same projection if the crs differs. First plot different crs, and then abstract it into a code chunk with a variable for the default crs used in a project.

maybe later use it to trim outliers with arg `trim_outliers` and `if isTRUE(trim_outlies){trim 'em}`

### Nested lists

Story: we want to assess change in groundwater level across all stations and plot it on a map. Plot beta_1. How we can get this with iteration. Begin with one linear model on one station. Then use lists. Introduce: Nested lists (explain list-columns) (linear models - slope of many dataframes)



### When to write a function

If you copy paste a workflow more than 3 times, write a function
reproducible workflows

Keeping other users in mind, using `stop()`


args: start date, end date (for the filter pre-lm), trim_outliers (to illustrate conditional execution)

- make one plot, then function 

- write one data set, then function for it all

imagemagik the plots into a gif


safely(), possibly() - toy cases to demo these functions



### When to define a pipeline

Analogy of watering a garden of plants one by one, or having one drip irrigation line. 

How and when to use `source(my_script.R)` to abstract out functions and other preprocessing pipelines to simplify scripts. The idea of scripts that build on one another in an increasingly complex project.

Take example above and make a control.R - to be used later with task scheduler or cron - we know this data updates monthly, so set it there. Imagine for flow data, this can be every day! Data frequency, latency.  


### Task scheduling


Here's how to use task scheduler and cron



### Logging

When things are running without your supervision, things may break so it's a good idea to print information whenever the job kicks off so you can review this information in case something doesn't go as planned. This is known as **logging**, which is cornerstone in software engineering, but less so in data science. Logging becomes important for larger jobs with potential to break, or when inputs may sufficiently change in such a way that breaks code.  


<br>  

## When to not automate

As a data scientist, you'll often need to perform once-over ad-hoc analyses. You may or may not need to return to these analyses, or components of the analyses. Automation takes time, and just because we have the tools to automate doesn't mean that we should. Whether or not you should automate depends on how often you anticipate the workflow needs to be re-evaluated. The more the workflow needs to be run, the more time we can justify spending on firming up the reproducibility and automation potential of program. When it's not immediately apparant that automation is necessary (e.g., "a daily report of streamflow automatically scraped from 1,000 sensors") a general rule of thumb for whether or not to automate may boil down to the following considerataion:

> Will the time saved in the future by automation exceed the time spent automating the process now? If so, automate.  

(ref:automate-xkcd2-fig) *Figure caption TBD*

```{r time-fig, eval=TRUE, echo=FALSE, out.width='75%',fig.cap='(ref:automate-xkcd2-fig)'}
knitr::include_graphics(here("images", "is_it_worth_the_time.png"))
```



Don't underestimate the time it can take to automate either. 

(ref:automate-xkcd-fig) *Figure caption TBD*

```{r no-automate-fig, eval=TRUE, echo=FALSE, out.width='75%',fig.cap='(ref:automate-xkcd-fig)'}
knitr::include_graphics(here("images", "automation_xkcd.png"))
```




<br>

## Code as instructions

We've all done a point-and-click analysis before that becomes unwieldy, large, and "too big to fail." In other words, we cross our fingers and hope we'll never have to repeat the analysis because either: 

1. it took so long to complete that it would be unimaginable to spend that same amount of time repeating it  
2. certain parts of the workflow were subjective, so repeating it would yield a different result  
3. repeating one part of the workflow would invalidate downstream components and derivative products of the workflow, creating even more work  

Code is best thought of as a set of instructions. No matter the language the code is in, whether it's `R`, `Python`, or another language for data science, at its most fundamental level, the code represents a linear set of instructions to carry out on a computer. Coding is the automation of routine point-and-click tasks, and a good program (e.g., `.R` script) will replace the need to click thrugh an analysis. Moreover, as data volume (size) and frequency (timing) increase, many point-and-click software tools are stretched to their limits and become slow or non-responsive, but lower-level code still works, and usually works much faster.


<br>

## Additional resources

- cron blog posst
- windows task sceduler link
- something from efficient R?




<br> 

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

<a href="m_iteration.html" class="btn btn-secondary" style="float: left">Previous module:<br>Iteration</a> <a href="m_parameterized_reports.html" class="btn btn-secondary" style="float: right;">Next module:<br>Parameterized reports</a>
