---
title: "Reproducible workflows and automation"
description: | 
   How to make computers do your bidding  
output: 
  distill::distill_article:
    toc: true
---

```{r setup, include=FALSE, purl=FALSE, message=FALSE}

library(knitr)
library(glue)
library(here)

```

::: {.obj}
**Learning objectives**

-   Practice functional programming iterative workflows to power reproducible analyses
-   Understand how to use nested lists within dataframes
-   Practice conditional execution
-   Discuss task schedulers and strategies to fully automate workflows\
:::

<br>

## Reproducible workflows and automation

In this module, we extend lessons in functional programming from the previous [module on iteration](m_iteration.html), as well as best practices discussed in the [project management module](m_project_management.html) to approach a case study. Specal attention is paid to automation.

As you expand your ability to write `R` code, **automation** becomes possible. "Automation" can mean different things, but generally refers to the process of replacing time that you would spend on a workflow with a computer program, or an `R` script in the context of this course. The beauty of automation is that you can create workflows that would be nearly impossible to complete in a human lifetime, and run them thousands of times, all while you're out on a coffee break. In some circles, this is sometimes jokingly referred to as, "making computers do your bidding." Automation happens whenever you write a program that saves you time, freeing you to do other things.

The very act of writing an `R` script (or program) to perform analyses is automation, because you have automated the need to point and click through various tasks and type instructions at particular times in a traditional point-and-click workflow.

However, the `R` program still needs a human to run it, and perhaps you have not one, but a chain of 5 scripts that are evaluated sequentially to import, clean, model, visualize, and output data. Someone needs to run the program. Another step up the automation ladder is to wrap these 5 scripts in a single program that calls them all in sequence, like a master switch. Instead of running 5 scripts, now we only need to run 1.

There is still automation potential. We can write another program to call the "master switch" at specified times, fully remove ourselves from the pipeline, and rest assured that our program will run all on its own (although it may require maintenance from time to time).

(ref:automate-fig) *Automation is like watering a row of plants. Each program is like a bucket that waters one plant at a time. Wrapping the programs into a single "master switch" control script is like installing drip irrigation so that all plants are watered at once. In order to fully automate, we can set a task scheduler, which is like adding a timer onto our drip irrigation system.*

```{r automate-fig, eval=TRUE, echo=FALSE, out.width='100%', fig.cap='(ref:automate-fig)'}
knitr::include_graphics(here("images", "automation.png"))
```

We should always strive for at least a degree of automation represented by the left-most figure, with a set of programs for each step in our workflow, and functions abstracted out of these programs[^1] to improve readability and project organization.

[^1]: As discussed in the project management module, [abstracting functions from code](m_project_management.html#abstracting-functions-from-code) is a best practice of defining functions and storing them in separate scripts to keep your workspace easier to read and de-bug.

Whether or not to write wrapper programs or configure task schedulers depends on the needs of the workflow. Guidance on when and when to not automate will be discussed in this module.

To demonstrate varying degrees of automation in a real-life example and extend our practice of functional programming, we will focus on a case study using the DWR's Periodic Groundwater Level Database.

<br>

## Case Study: groundwater level trend analysis

The California Department of Water Resources publishes a routine report on groundwater level conditions that shows groundwater level trends on a point-by-point basis. Short-term (i.e., 1, 3, and 5 year) and long-term trends (i.e., 10 and 20 years) are displayed in the report.

(ref:ca-dwr-fig) *The ["California Groundwater Conditions Update - Spring 2020", DWR (2020)](https://water.ca.gov/-/media/DWR-Website/Web-Pages/Programs/Groundwater-Management/Data-and-Tools/Files/Maps/Groundwater-Level-Change/DOTMAP_Reports/Spring-2020-Groundwater-DOTMAP-Report.pdf) demonstrates the rate and direction of groundwater level change over different time periods, shown here is the 20000-2020 trend.*

```{r ca-dwr, eval=TRUE, echo=FALSE, fig.cap='(ref:ca-dwr-fig)'}
knitr::include_graphics(here("images", "dwr_gwl.png"))
```

Let's imagine that our task is to assess change in groundwater level across all stations and plot it on a map, just as we see in the report. In the sections that follow we will calculate groundwater level change trends at representative monitoring points in the [Periodic groundwater level database](https://data.cnra.ca.gov/dataset/periodic-groundwater-level-measurements) we have been using in this course, and demonstrate best practices in creating a reproducible, automatable pipeline. Along the way, we will also cover key concepts including conditional execution, functional programming with nested lists, tips for writing helpful functions, task scheduling, and logging.

To begin, let's import data and join it. If your internet connection is slow, use `d <- read_csv(here("data", "gwl_sac.csv"))` to skip the step below and read in the data. If you want to see how to download this data, follow along with the code below[^2]:

[^2]: Note that over time, the URLS in this code may change, and the data will certainty grow in volume. Thus, when comparing the output of the code to the data provided with this course, there may be differences because this code always grabs the most up-to-date data, which will probably contain more records than what we provide.

```{r import, eval = FALSE}
library(tidyverse)
library(here)

# urls for data
base_url <- 
  "https://data.cnra.ca.gov/dataset/dd9b15f5-6d08-4d8c-bace-37dc761a9c08/resource/"
urls <- paste0(
  base_url, 
  c("af157380-fb42-4abf-b72a-6f9f98868077/download/stations.csv",
    "bfa9f262-24a1-45bd-8dc8-138bc8107266/download/measurements.csv",
    "f1deaa6d-2cb5-4052-a73f-08a69f26b750/download/perforations.csv")
)

# create directory and define paths to save downloaded files
dir.create(here("data", "pgwl"))
files_out <- here("data", "pgwl", basename(urls))

# download files
walk2(urls, files_out, ~download.file(.x, .y))

# read and combine - use datatable for speed and memory management
d <- map(files_out, ~data.table::fread(.x)) %>% 
  reduce(left_join, by = "SITE_CODE")

# d contains ~2.3 million observations: filter to Sacramento County
d <- filter(d, COUNTY_NAME == "Sacramento")
```

<aside>

<br>

We use copied URLs here, but for large, standardized websites you can use [`{rvest}` and SelectorGadget](https://rvest.tidyverse.org/articles/articles/selectorgadget.html) to automate scraping URLs and other data. We also used `purrr::reduce()` to recursively `left_join()` a list of dataframes. This is a powerful concept, because the alternative is to manually `left_join()` each list element. Read up on `?reduce` to learn more.

<br>

</aside>

```{r, read-sac-co-gwl, echo = FALSE, message=FALSE, error=FALSE}
library(tidyverse)
library(here)
d <- read_csv(here("data", "gwl", "gwl_sac.csv"))
```

Now that we have our Sacramento data in `R` either by downloading it with the code above, or by reading it in with `d <- read_csv(here("data", "gwl_sac.csv"))`, let's transform it to an `sf` object and plot just to sanity check that we're indeed looking at Sacramento County groundwater levels.

```{r gwl-mapview, layout = "l-page"}
library(sf)
library(mapview)
mapviewOptions(fgb = FALSE)

# select only columns we will use in this analysis
d <- d %>% 
  select(SITE_CODE, MSMT_DATE, WSE, LONGITUDE, LATITUDE, WELL_DEPTH)

# convert to sf object
d <- st_as_sf(d, coords = c("LONGITUDE", "LATITUDE"), crs = 4269) 

# take the first observation per SITE_CODE since they all have 
# the same location, and plot
group_by(d, SITE_CODE) %>% 
  slice(1) %>% 
  ungroup() %>% 
  mapview(popup = NULL)
```

### Nested lists

Recall that we want to assess change in groundwater level across all stations and plot it on a map. In other words, assume we have the linear model describing a set of groundwater elevations ($\hat{Y}_i$), as a function of time ($X_i$), proportional to some unknown y-intercept ($\beta_0$) and coefficient ($\beta_1$) for a set of observations at times $i = 1, ..., n$:

$$
\hat{Y}_i = \beta_0 + \beta_1(X_i) + \epsilon_i
$$

Time alone is actually a poor predictor of groundwater level (and shouldn't be interpreted in a strict statistical sense), but for our purposes, the linear model above is useful because the $\beta_1$ coefficient can be interpreted as the average groundwater level trend at a monitoring site over some time frame. If $\beta_1 > 0$, the average groundwater level is increasing, and if $\beta_1 < 0$, it is decreasing. The magnitude of $\beta_1$ tells us by how much the groundwater level is increasing or decreasing and can be plotted per monitoring site, and compared to other monitoring sites to understand the spatial distribution of groundwater level change in a region. In this example, our region of interest is Sacramento County, but as we will demonstrate, we will create a function that allows us to apply this workflow to any county, or even the entire state.  

To apply a linear model to each monitoring site, we'll use the `lm()` function in base `R`. If we apply the function to our entire dataset, we'll end up with one linear model of all groundwater levels in Sacramento county:

```{r gwl-1}
ggplot(d) +
  geom_line(aes(MSMT_DATE, WSE, group = SITE_CODE), alpha = 0.5) +
  geom_smooth(aes(MSMT_DATE, WSE), method = "lm", se = FALSE)
```

Let's filter out that one outlier (groundwater levels don't just spike by hundreds of feet so rapidly like that), filter the date range of the data to the last 20 years, and replot. We can see a slight negative groundwater level trend over the basin, but this spatial average may be skewed by monitoring stations with more data than others and increased data density in more recent years. 

```{r gwl-2}
# recent data without the obviously erroneous outlier
d_recent <- filter(d, WSE >-200 & MSMT_DATE >= lubridate::ymd("2000-01-01"))

ggplot(d_recent) +
  geom_line(aes(MSMT_DATE, WSE, group = SITE_CODE), alpha = 0.5) +
  geom_smooth(aes(MSMT_DATE, WSE), method = "lm", se = FALSE)
```

What we need is a linear model for each and every one of the individual stations in this dataset. For example, here are a few of the monitoring sites.

```{r gwl-3}
d_recent %>% 
  filter(SITE_CODE %in% unique(d$SITE_CODE)[1:7]) %>% 
  ggplot() +
  geom_line(aes(MSMT_DATE, WSE)) +
  geom_smooth(aes(MSMT_DATE, WSE), method = "lm", se = FALSE) +
  facet_wrap(~SITE_CODE)
```

`{ggplot2}` is great for visualizing what we are looking for. Now, it's time to calculate those numbers with `lm()`. We will use `{broom}` to tidy up model results in a clean dataframe. Note that because the class of our `MSMT_DATE` column is `"POSIXct"`, the units returned for our coefficient $\beta_1$ will be in $ft/sec$ rather than the the more intuitive $ft/yr$, thus we define a function to handle this conversion. For the entire dataset the average annual change in groundwater level over the 20 year timeframe is:

```{r gwl-sac}
# convert from seconds to years
sec_to_yr <- function(sec){
  yr <- sec * 31557600
  return(yr)
}

# create the linear model and view output
m <- lm(WSE ~ MSMT_DATE, data = d_recent) 

m

# tidy the model output with broom and convert ft/sec to ft/yr
broom::tidy(m) %>% 
  slice(2) %>% 
  mutate(estimate_ft_yr = sec_to_yr(estimate))
```

From above, we see that the average annual decline is `r round(sec_to_yr(coefficients(m)["MSMT_DATE"]), 2)` $ft/yr$.  

We could turn this linear model call into a function and apply over a list of dataframes created by `split(d, d$SITE_CODE)` (or the tidyverse equivalent, `group_split(d, SITE_CODE)`), but there's a better way with functional programming.

We will use nested lists, specifically with the `tidyr::nest()` function, which allows us to convert a dataframe grouped by some variable (in our case it will be `SITE_CODE`) into a dataframe with a column that "nests" the data for each unique grouping variable. For instance:

```{r nest}
l <- d_recent %>% 
  group_by(SITE_CODE) %>% 
  nest() 
l
```

The resulting dataframe has a number of rows equal to the length of unique values in the `SITE_CODE` column, and a column called `data` with nested dataframes, one for each `SITE_CODE` (grouping variable). Let's inspect the first dataframe:;

```{r nest2}
l$data[[1]]
```

The beauty of having data nested in this way is that we can mutate new columns on the dataframe based on the nested `data` column by mapping over each of the dataframes:

```{r nest3}
d_recent %>% 
  group_by(SITE_CODE) %>% 
  # nest the data per SITE_CODE grouping variable
  nest() %>% 
  mutate(
    # map a linear model onto the data
    model = map(data, ~lm(WSE ~ MSMT_DATE, data = .x))
  )
```

Now we have a linear model output object per `SITE_CODE`, but let's not stop there. Let's also extract the intercept ($\beta_1$), mutate new columns indicating the direction and magnitude of the rate, convert units to $ft/yr$, and `unnest()` the data. 

```{r nest4}
d_recent <- d_recent %>% 
  group_by(SITE_CODE) %>% 
  # nest the data per SITE_CODE grouping variable
  nest() %>% 
  mutate(
    # map a linear model across data, extract slope magnitude & direction
    model     = map(data, ~lm(WSE ~ MSMT_DATE, data = .x)),
    b1        = map(model, ~coefficients(.x)[["MSMT_DATE"]]),
    b1        = map(b1, ~sec_to_yr(.x)),
    direction = map(b1, ~ifelse(.x >= 0, "increase", "decline")),
    # duration of the data in units of years so we know how the period
    # over which the linear model is estimated
    length_yr = map(data, ~as.numeric(diff(range(.x$MSMT_DATE))) / 365)
  ) %>% 
  # unnest all columns so we can access values
  unnest(c(data, b1:length_yr)) %>% 
  select(-model)
```

Take a moment to `View(d_recent)` and inspect the new columns. 

Now we can visualize trends for each station like before. Note hat not all records are a full 20 years long!  

```{r nest5}
d_recent %>% 
  filter(SITE_CODE %in% unique(d$SITE_CODE)[1:7]) %>% 
  ggplot() +
  geom_line(aes(MSMT_DATE, WSE)) +
  geom_smooth(aes(MSMT_DATE, WSE, color = direction), method = "lm", se = FALSE) +
  facet_wrap(~SITE_CODE) +
  labs(color = "Trend")
```

Let's not forget that these trends have a spatial component. Let's only visualize sites with a `duration_yr` of 5 years or more.  

```{r nest6}
# slice the first observation per SITE_ID (for location) and 
# re-convert to sf which is lost during nest and unnest
d_map <- d_recent %>% 
  group_by(SITE_CODE) %>% 
  slice(1) %>% 
  ungroup() %>% 
  st_as_sf() %>% 
  filter(length_yr >= 5)

# create a bin for trend magnitude that matches DWR bins (Fig 2)
d_map$bin <- cut(
  d_map$b1, 
  breaks = c(-1000, -2.5, 0, 2.5, 1000), 
  labels = c("Increase > 2.5 ft/yr", "Increase 0 - 2.5 ft/yr", 
             "Decrease 0 - 2.5f ft/yr", "Decrease > 2.5 ft/yr")
)
  
# sacramento county polygon
sac <- st_read(here("data", "shp", "sac", "sac_county.shp")) %>% 
  st_transform(st_crs(d_map))

# plot direction
ggplot() +
  geom_sf(data = sac) +
  geom_sf(data = d_map, aes(fill = direction), pch = 21, size = 2) +
  guides(fill = FALSE) +
  theme_void()

# plot magnitude bins
ggplot() +
  geom_sf(data = sac) +
  geom_sf(data = d_map, aes(color = bin), size = 2, alpha = 0.8) +
  rcartocolor::scale_color_carto_d("",palette = "TealRose") +
  theme_void()
```

The map above suggest there are only 3 locations in the basin that show an absolute change of more than 2.5 $ft/yr$. Most of the longer term monitoring locations have an annual absolute rate of change within 0-2.5 $ft/yr$. Without knowing exactly which sites were selected in Sacramento county, a quick comparison with Figure \@ref(fig:ca-dwr) suggests that our calculated groundwater level trends are similar in magnitude.


### How to write helpful functions

Everything we've done until this point is valuable analysis, but is it reproducible? In the sense that we can re-run these scripts on new data, yes, but we can apply some principles of good project management to refactor our code into functions and programs (scripts) that are easy to re-run (Figure \@ref(fig:cautomate-fig), left-most situation). We will go one step further and wrap everything into a control script (Figure \@ref(fig:cautomate-fig), center situation), and in the next section, we'll discuss how to fully automate (Figure \@ref(fig:cautomate-fig), right-most situation).  

In the spirit of "don't repeat yourself" (DRY), it's often said:  

> If you copy paste a workflow more than 3 times, write a function.  

Recall that in this case study, we want to reevaluate groundwater level trends twice per year, once in spring and once in fall. We decide to save time in the future by abstracting these tasks into a set of functions and a central control script. We also know that others will need to use this same workflow, so we strive to turn our analysis into a reproducible tool for others and include helpful messaging, errors, and conditional execution (i.e., `if` statements) in the code.  

Let's begin with the download step. Save this as `functions/f_wcr_download.R`. Notice that we add a `dir_out` argument that allows a user to specify a directory to save files to. If this path doesn't exist, we create it. Also, we allow users to specify the urls, so if case they change in the future, this function need not change.  

```{r f-wcr-download}

# download wcr data
f_wcr_download <- function(dir_out, urls){

  # create directory and define paths to save downloaded files
  if(! dir.exists(dir_out)){ 
    dir.create(dir_out) 
  }
  
  # output file paths
  files_out <- file.path(dir_out, basename(urls))
  
  # download files
  walk2(urls, files_out, ~download.file(.x, .y))
  
}

```

Next, the import function should be saved as `functions/f_wcr_import.R`. Let's design it so that the user provides a valid county, and the function returns a helpful message in case they don't provide a valid county. 

```{r f-wcr-import}

# import downloaded wcr data
f_wcr_import <- function(county, files_in){
  # valid counties from entire dataset, i.e., 
  # dput(sort(unique(d$COUNTY_NAME)))
  counties_valid <-
    c("Alameda", "Alpine", "Amador", "Butte", "Calaveras", "Colusa", 
      "Contra Costa", "Del Norte", "El Dorado", "Fresno", "Glenn", 
      "Humboldt", "Imperial", "Inyo", "Kern", "Kings", "Klamath, OR", 
      "Lake", "Lassen", "Los Angeles", "Madera", "Marin", "Mariposa", 
      "Mendocino", "Merced", "Modoc", "Mono", "Monterey", "Napa", "Nevada", 
      "Orange", "Placer", "Plumas", "Riverside", "Sacramento", "San Benito", 
      "San Bernardino", "San Diego", "San Francisco", "San Joaquin", 
      "San Luis Obispo", "San Mateo", "Santa Barbara", "Santa Clara", 
      "Santa Cruz", "Shasta", "Sierra", "Siskiyou", "Solano", "Sonoma", 
      "Stanislaus", "Sutter", "Tehama", "Tulare", "Tuolumne", "Ventura", 
      "Yolo", "Yuba", "ALL")
  
  # ensure input county is valid before reading in data
  if(! county %in% counties_valid) {
    stop(
      glue("Provided county is invalid. Did you mean one of the following?
           {paste(counties_valid, collapse = ', ')}"),
      call. = FALSE
    )
  }
  
  cat("Valid county provided, now proceeding to read data...", "\n")
  
  # read and combine - use datatable for speed and memory management
  d <- map(files_in, ~data.table::fread(.x)) %>% 
    reduce(left_join, by = "SITE_CODE") 
  
  cat("Files read and combined...", "\n")
  
  # if a county is supplied (not requesting ALL the data), filter to it
  if(county != "ALL"){
    cat("Filtering to", county, "county...", "\n")
    d <- filter(d, COUNTY_NAME == county)
  }
  
  # select only relevant columns & convert to sf object
  cat("Selecting relevant columns...", "\n")
  d <- d %>% 
    select(SITE_CODE, MSMT_DATE, WSE, LONGITUDE, LATITUDE, WELL_DEPTH) %>% 
    st_as_sf(d, coords = c("LONGITUDE", "LATITUDE"), crs = 4269) 
  
  cat("Compelete!", "\n")
  return(d)
}

```

Notice the use of conditional execution (`if()` statements that check if certain criteria apply before proceeding), coupled with `stop()` functions that terminate the function call. It doesn't make sense to read in an entire dataframe and filter by a character sting that's not present in the data, so we check for that condition before proceeding. We also provide helpful error message in case someone enters an incorrect county. We added a valid county called "ALL" which if passed to the function, doesn't filter the data to a county, such that the entire dataset is returned. All throughout, we added helpful `cat()` messages so that as this function is chugging along, messages are printed to the console to let us know what's happening under the hood. This also helps with debugging and logging (as we will discuss later).

To verify our function works as expected, let's trigger an error message by asking it to return groundwater levels from a county that doesn't exist in the vector of valid counties. 

```{r f-test-error, eval = FALSE}
f_wcr_import(county = "Tatooine")
```

```
Error: The provided county is invalid. Did you mean one of:
Alameda, Alpine, Amador, Butte, Calaveras, Colusa, Contra Costa, Del Norte, 
El Dorado, Fresno, Glenn, Humboldt, Imperial, Inyo, Kern, Kings, Klamath, OR, 
Lake, Lassen, Los Angeles, Madera, Marin, Mariposa, Mendocino, Merced, Modoc, 
Mono, Monterey, Napa, Nevada, Orange, Placer, Plumas, Riverside, Sacramento, 
San Benito, San Bernardino, San Diego, San Francisco, San Joaquin, San Luis Obispo, 
San Mateo, Santa Barbara, Santa Clara, Santa Cruz, Shasta, Sierra, Siskiyou, 
Solano, Sonoma, Stanislaus, Sutter, Tehama, Tulare, Tuolumne, Ventura, Yolo, Yuba, ALL
```

Next, we need to clean our data. You can imagine a much more extensive cleaning process for another dataset, but in our simple example, all we want to do is provide the user with the option to set a start and end time to filter the data by. Importantly, we want to ensure that the date arguments passed to the function are valid "Date", "POSIXct", or "POSIXt" object classes so a proper filter occurs. 

Save the following code as `functions/f_wcr_clean.R`: 

```{r f-wcr-clean}
# clean wcr data
f_wcr_clean <- function(df, start_date = NULL, end_date = NULL){

  # valid date classes
  valid_class <- c("Date", "POSIXct", "POSIXt")

  # verify correct date class
  if(! is.null(start_date) & ! class(start_date)[1] %in% valid_class ){
    stop(
      glue(
        "Invalid `start_date` class, use: {paste(valid_class, collapse=', ')}",
      ),
      call. = FALSE
    )
  }
  if(! is.null(end_date) & ! class(end_date)[1] %in% valid_class ){
    stop(
      glue(
        "Invalid `end_date` class, use: {paste(valid_class, collapse=', ')}",
      ),
      call. = FALSE
    )
  }

  # if start and end date are NULL, use the min and max date
  if(is.null(start_date)){
    cat("Start date null... ")
    start_date <- min(df$MSMT_DATE, na.rm = TRUE)
    cat("using min date in dataframe:", as.character(start_date), "\n")
  }
  if(is.null(end_date)){
    cat("End date null... ")
    end_date <- max(df$MSMT_DATE, na.rm = TRUE)
    cat("using max date in dataframe:", as.character(end_date), "\n")
  }

  # filter to date range and clean
  df <- filter(df, MSMT_DATE >= start_date & MSMT_DATE <= end_date)

  return(df)
}
```

Notice that most of this function is focused not on the transformation, but rather, checking that the correct object classes are passed into the function, and sensibly imputing argument values if they're not provided by the user. The actual `filter()` statement only takes 1 line of code, but by the time we arrive at this point, we're fairly confident that the function will work as expected. The amount of time you should spend tuning functions like this depends on how much you anticipate others using them, and how much you want to constrain their use and messaging. Not every project requires this level of detail. 

Moving on, after cleaning, it's time to model. Save another function `functions/f_wcr_model.R` that calculates the linear model at each `SITE_CODE`:

```{r}
# build linear model for each unique SITE ID and
# add important columns (direction), beta_1 coefficient
f_wcr_model <- function(df){
  
  result <- df %>%
    group_by(SITE_CODE) %>%
    # nest the data per SITE_CODE grouping variable
    nest() %>%
    mutate(
      # map a linear model across data, extract slope magnitude & direction
      model     = map(data, ~lm(WSE ~ MSMT_DATE, data = .x)),
      b1        = map(model, ~coefficients(.x)[["MSMT_DATE"]]),
      b1        = map(b1, ~sec_to_yr(.x)),
      direction = map(b1, ~ifelse(.x >= 0, "increase", "decline")),
      # duration of the data in units of years so we know how the period
      # over which the linear model is estimated
      length_yr = map(data, ~as.numeric(diff(range(.x$MSMT_DATE))) / 365)
    ) %>%
    # unnest all columns so we can access values
    unnest(c(data, b1:length_yr)) %>%
    select(-model)

  return(result)
}
```

Finally, we need to visualize and save these data. Add another function `functions/f_wcr_visualize.R`

```{r}

```


Now that we have functions defined, let's define modules that download, import and clean, model, visualize, and write. 


```{r, eval = FALSE}
list.files(here::here("functions"), pattern = "wcr", full.names = TRUE) %>% 
  walk(~source(.x))

# ------------------------------------------------------------------------
# arguments needed by functions

# urls for data to download
base_url <- 
  "https://data.cnra.ca.gov/dataset/dd9b15f5-6d08-4d8c-bace-37dc761a9c08/resource/"
urls <- paste0(
  base_url, 
  c("af157380-fb42-4abf-b72a-6f9f98868077/download/stations.csv",
    "bfa9f262-24a1-45bd-8dc8-138bc8107266/download/measurements.csv",
    "f1deaa6d-2cb5-4052-a73f-08a69f26b750/download/perforations.csv")
)

# directory to save downloaded data
dir_out <- here("data", "pgwl")

# input location for files
files_in <- file.path(dir_out, basename(urls))

# ------------------------------------------------------------------------
# download wcr data
f_wcr_download(dir_out, urls)

# import wcr data, clean, and model
d <- f_wcr_import(county = "Sacramento", files_in = files_in) %>%
  f_wcr_clean(start_date = ymd("2000-01-01"), 
              end_date   = ymd("2021-01-01")) %>% 
  f_wcr_model()

# visualize and export 
```



Keeping other users in mind, using `stop()` and `cat()` messages.

args: start date, end date (for the filter pre-lm), trim_outliers (to illustrate conditional execution)

-   make one plot, then function

-   write one data set, then function for it all

imagemagik the plots into a gif

safely(), possibly() - toy cases to demo these functions

### Conditional execution

If statements within a function

Example for transforming spatial data to the same projection if the crs differs. First plot different crs, and then abstract it into a code chunk with a variable for the default crs used in a project.

-   maybe later use it to trim outliers with arg `trim_outliers` and `if isTRUE(trim_outlies){trim 'em}`

-   another if statement for argument `county`, when, if NULL, does the operation on th entire dataset.

### When to define a pipeline

Analogy of watering a garden of plants one by one, or having one drip irrigation line.

How and when to use `source(my_script.R)` to abstract out functions and other preprocessing pipelines to simplify scripts. The idea of scripts that build on one another in an increasingly complex project.

Take example above and make a control.R - to be used later with task scheduler or cron - we know this data updates monthly, so set it there. Imagine for flow data, this can be every day! Data frequency, latency.

### Task scheduling

Here's how to use task scheduler and cron

### Logging

When things are running without your supervision, things may break so it's a good idea to print information whenever the job kicks off so you can review this information in case something doesn't go as planned. This is known as **logging**, which is cornerstone in software engineering, but less so in data science. Logging becomes important for larger jobs with potential to break, or when inputs may sufficiently change in such a way that breaks code.

<br>

## When to not automate

As a data scientist, you'll often need to perform once-over ad-hoc analyses. You may or may not need to return to these analyses, or components of the analyses. Automation takes time, and just because we have the tools to automate doesn't mean that we should. Whether or not you should automate depends on how often you anticipate the workflow needs to be re-evaluated. The more the workflow needs to be run, the more time we can justify spending on firming up the reproducibility and automation potential of program. When it's not immediately apparant that automation is necessary (e.g., "a daily report of streamflow automatically scraped from 1,000 sensors") a general rule of thumb for whether or not to automate may boil down to the following considerataion:

> Will the time saved in the future by automation exceed the time spent automating the process now? If so, automate.

(ref:automate-xkcd2-fig) *Figure caption TBD*

```{r time-fig, eval=TRUE, echo=FALSE, out.width='75%',fig.cap='(ref:automate-xkcd2-fig)'}
knitr::include_graphics(here("images", "is_it_worth_the_time.png"))
```

Don't underestimate the time it can take to automate either.

(ref:automate-xkcd-fig) *Figure caption TBD*

```{r no-automate-fig, eval=TRUE, echo=FALSE, out.width='75%',fig.cap='(ref:automate-xkcd-fig)'}
knitr::include_graphics(here("images", "automation_xkcd.png"))
```

<br>

## Code as instructions

We've all done a point-and-click analysis before that becomes unwieldy, large, and "too big to fail." In other words, we cross our fingers and hope we'll never have to repeat the analysis because either:

1.  it took so long to complete that it would be unimaginable to spend that same amount of time repeating it\
2.  certain parts of the workflow were subjective, so repeating it would yield a different result\
3.  repeating one part of the workflow would invalidate downstream components and derivative products of the workflow, creating even more work

Code is best thought of as a set of instructions. No matter the language the code is in, whether it's `R`, `Python`, or another language for data science, at its most fundamental level, the code represents a linear set of instructions to carry out on a computer. Coding is the automation of routine point-and-click tasks, and a good program (e.g., `.R` script) will replace the need to click thrugh an analysis. Moreover, as data volume (size) and frequency (timing) increase, many point-and-click software tools are stretched to their limits and become slow or non-responsive, but lower-level code still works, and usually works much faster.

<br>

## Additional resources

-   cron blog posst
-   windows task sceduler link
-   something from efficient R?

<br>

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

<a href="m_iteration.html" class="btn btn-secondary" style="float: left">Previous module:<br>Iteration</a> <a href="m_parameterized_reports.html" class="btn btn-secondary" style="float: right;">Next module:<br>Parameterized reports</a>
